{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\moren\\AppData\\Local\\Temp\\ipykernel_17000\\511313907.py:4: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Specify the path to the directory containing the ChromeDriver executable\n",
    "chrome_driver_directory = \"C:/Users/moren/Downloads/chromedriver-win64\" #insert your own path here #User moreno: 'moren'\n",
    "\n",
    "# Add the ChromeDriver directory to the PATH environment variable\n",
    "os.environ[\"PATH\"] += os.pathsep + chrome_driver_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1 dataframes after match ID: 4089693\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 2 dataframes after match ID: 4089694\n",
      "Concatenating 2 dataframes.\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4089693 # First Game ID of the season\n",
    "end_match_id = 4089824 # Adjust this according to your requirement\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store all lineup data\n",
    "lineups_df = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "all_lineup_stats_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "    \n",
    "    \n",
    "    ## SCRAPING ##\n",
    "    \n",
    "    # Extract the gameday information from the top of the page\n",
    "    #gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]/a[1]').text\n",
    "    gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]').text\n",
    "\n",
    "    # Extract the home and away club names from the 'title' attribute\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "\n",
    "\n",
    "    # Function to extract data from a table given its rows\n",
    "    def extract_table_data(table_rows, club_name):\n",
    "        positions = []\n",
    "        players = []\n",
    "        ages = []\n",
    "        market_values = []\n",
    "        club_names = [club_name] * (len(table_rows) // 3) # There's a club name for each player row\n",
    "        gamedays = [gameday] * (len(table_rows) // 3)  # Same gameday for all players in the match\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        for i in range(0, len(table_rows), 3):  # Increment by 3 for each player's data set\n",
    "            cells = table_rows[i].find_elements(By.TAG_NAME, \"td\")\n",
    "            player_info = cells[1].text\n",
    "            name_age_parts = player_info.split(' (')\n",
    "            player_name = name_age_parts[0].strip()\n",
    "            age_part = name_age_parts[1] if len(name_age_parts) > 1 else ''\n",
    "            age_match = re.search(r'(\\d+) years old', age_part)\n",
    "            age = age_match.group(1) if age_match else None\n",
    "            position_market_value = cells[4].text\n",
    "            if ', ' in position_market_value:\n",
    "                position, market_value = position_market_value.split(', ')\n",
    "            else:\n",
    "                position = position_market_value\n",
    "                market_value = None\n",
    "        \n",
    "            players.append(player_name)\n",
    "            ages.append(age)\n",
    "            positions.append(position)\n",
    "            market_values.append(market_value)\n",
    "    \n",
    "        return pd.DataFrame({\n",
    "            'Position': positions,\n",
    "            'Player': players,\n",
    "            'Age': ages,\n",
    "            'Market Value': market_values,\n",
    "            'Club': club_names,\n",
    "            'Gameday': gamedays,\n",
    "        })\n",
    "    all_tables_df = []\n",
    "\n",
    "    # XPath or CSS Selector for each table\n",
    "    tables_xpaths = {\n",
    "        'starting_lineup_home': '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[1]/table', \n",
    "        'substitutes_home': '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[1]/table',\n",
    "        'starting_lineup_away': '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[1]/table',\n",
    "        'substitutes_away': '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[1]/table'\n",
    "    }\n",
    "\n",
    "    all_tables_df = []\n",
    "\n",
    "    # Loop through the table paths and extract data\n",
    "    for key, xpath in tables_xpaths.items():\n",
    "        try:\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            team_type = 'Home' if 'home' in key else 'Away'\n",
    "            club_name = home_club_name if 'home' in key else away_club_name\n",
    "            df = extract_table_data(rows, club_name)  # Your custom function to extract data from rows\n",
    "            df['H/A'] = team_type\n",
    "            df['Status'] = 'Starting' if 'starting' in key else 'Substitute'\n",
    "            all_tables_df.append(df)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Table not found for {key} in match ID: {match_id}, skipping.\")\n",
    "            continue  # Skip this iteration if table is not found\n",
    "\n",
    "    # Combine all dataframes from the current page into lineups_df\n",
    "    if all_tables_df:  # Check if there's any data to concatenate\n",
    "        temp_df = pd.concat(all_tables_df, ignore_index=True)\n",
    "        temp_df['Match ID'] = match_id  # Add the match_id to every row in temp_df\n",
    "    \n",
    "        # Assuming lineups_df is defined somewhere above as the final dataframe\n",
    "        lineups_df = pd.concat([lineups_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Extract the home and away club names\n",
    "    home_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]')\n",
    "    home_club_name = home_club_name_element.get_attribute(\"title\")\n",
    "    away_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]')\n",
    "    away_club_name = away_club_name_element.get_attribute(\"title\")\n",
    "\n",
    "    # Extract home and away managers' names using the updated XPaths\n",
    "    home_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[1]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    home_manager_name = home_manager_element.text\n",
    "    away_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[2]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    away_manager_name = away_manager_element.text\n",
    "\n",
    "    # Extract additional information for both home and away teams with exception handling\n",
    "    try:\n",
    "        foreigners_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_away = \"N/A\"\n",
    "\n",
    "\n",
    "\n",
    "    # Function to clean the extracted data by removing preceding text\n",
    "    def clean_data(text, keep_eur_sign=False):\n",
    "        if keep_eur_sign:\n",
    "            # Directly slice away the preceding text if it follows a known pattern\n",
    "            if 'Purchase value: ' in text:\n",
    "                return text.replace('Purchase value: ', '')\n",
    "            elif 'Total MV: ' in text:\n",
    "                return text.replace('Total MV: ', '')\n",
    "        else:\n",
    "            # Using regex to find numeric values or percentages and return them for other columns\n",
    "            match = re.search(r'\\d+(\\.\\d+)?%?', text)\n",
    "            return match.group(0) if match else text\n",
    "    \n",
    "    # Create a DataFrame for the club and manager information along with the newly extracted data\n",
    "    lineups_stats_df = pd.DataFrame({\n",
    "        'Club': [home_club_name, away_club_name],\n",
    "        'H/A': ['Home', 'Away'],\n",
    "        'Manager': [home_manager_name, away_manager_name],\n",
    "        'Foreigners Starting': [clean_data(foreigners_starting_home), clean_data(foreigners_starting_away)],\n",
    "        'Foreigners Subs': [clean_data(foreigners_subs_home), clean_data(foreigners_subs_away)],\n",
    "        'Avg Age Starting': [clean_data(avg_age_starting_home), clean_data(avg_age_starting_away)],\n",
    "        'Avg Age Subs': [clean_data(avg_age_subs_home), clean_data(avg_age_subs_away)],\n",
    "        'Purchase Value Starting': [clean_data(purchase_value_starting_home, True), clean_data(purchase_value_starting_away, True)],\n",
    "        'Purchase Value Subs': [clean_data(purchase_value_subs_home, True), clean_data(purchase_value_subs_away, True)],\n",
    "        'Total Market Value Starting': [clean_data(total_market_value_starting_home, True), clean_data(total_market_value_starting_away, True)],\n",
    "        'Total Market Value Subs': [clean_data(total_market_value_subs_home, True), clean_data(total_market_value_subs_away, True)],\n",
    "        'Match ID': [match_id, match_id]\n",
    "    })\n",
    "\n",
    "    # Append the lineup stats dataframe for the current match to the list\n",
    "    all_lineup_stats_dfs.append(lineups_stats_df)\n",
    "\n",
    "    # Print the number of dataframes collected after each match\n",
    "    print(f\"Collected {len(all_lineup_stats_dfs)} dataframes after match ID: {match_id}\")\n",
    "\n",
    "\n",
    "# Before the concatenation, print out the number of dataframes to be concatenated\n",
    "print(f\"Concatenating {len(all_lineup_stats_dfs)} dataframes.\")\n",
    "\n",
    "# Concatenate all the lineup stats dataframes in the list\n",
    "final_lineup_stats_df = pd.concat(all_lineup_stats_dfs, ignore_index=True)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "lineups_df.to_csv('data/lineups_2023_2024_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lineup_stats_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lineup_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineups_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4089693  # First Game ID of the season\n",
    "end_match_id = 4089824  # Adjust this according to your requirement\n",
    "\n",
    "# Initialize an empty list to store all events dataframes\n",
    "all_events_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/index/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Handling the iframe and accept button if exists\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "        driver.switch_to.default_content()\n",
    "    except:\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "\n",
    "    ## SCRAPING ## \n",
    "\n",
    "    # Extracting club names\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div/div/div[1]/div[1]/div[2]/nobr/a').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div/div/div[2]/div[1]/div[2]/nobr/a').get_attribute(\"title\")\n",
    "\n",
    "    # Function to convert pixel values to minutes based on the pattern provided\n",
    "    def convert_px_to_minute(x_px, y_px):\n",
    "        # Remove any non-numeric characters and convert to integer\n",
    "        x_px = int(re.sub(r'[^\\d-]', '', str(x_px)))\n",
    "        y_px = int(re.sub(r'[^\\d-]', '', str(y_px)))\n",
    "    \n",
    "        # Convert negative values to positive\n",
    "        x_px = abs(x_px)\n",
    "        y_px = abs(y_px)\n",
    "    \n",
    "        unit_minutes = (x_px // 36) + 1\n",
    "        ten_minutes = (y_px // 36) * 10\n",
    "        timestamp = f\"{unit_minutes + ten_minutes}'\"\n",
    "        return timestamp\n",
    "\n",
    "\n",
    "    def extract_px_from_style(style_str):\n",
    "        # Use regular expression to find all pixel values in the style string\n",
    "        px_values = re.findall(r'-?\\d+px', style_str)  # Include optional minus sign\n",
    "    \n",
    "        # Check if there are at least two pixel values\n",
    "        if len(px_values) >= 2:\n",
    "            x_px, y_px = [int(px.strip('px')) for px in px_values[:2]]  # Take the first two values\n",
    "            return x_px, y_px\n",
    "        else:\n",
    "            # Handle the case when there are not enough values\n",
    "            return None, None  # You can return None or some default values\n",
    "\n",
    "\n",
    "    # Function to extract events with Remark Event adjustment\n",
    "    def extract_events(event_type_xpath, event_type, home_club_name, away_club_name):\n",
    "        try:\n",
    "            events_list = driver.find_element(By.XPATH, event_type_xpath)\n",
    "            events_items = events_list.find_elements(By.TAG_NAME, \"li\")\n",
    "            events_data = []\n",
    "\n",
    "            for item in events_items:\n",
    "                team = \"Home\" if \"heim\" in item.get_attribute(\"class\") else \"Away\"\n",
    "                club = home_club_name if team == \"Home\" else away_club_name\n",
    "\n",
    "                # Extract the style attribute for timestamp\n",
    "                style_str = item.find_element(By.XPATH, \".//div/div[1]/span\").get_attribute(\"style\")\n",
    "                x_px, y_px = extract_px_from_style(style_str)\n",
    "                timestamp = convert_px_to_minute(x_px, y_px)\n",
    "\n",
    "                player_event = \"N/A\"  # Default value if player name is not found\n",
    "                player_out = None  # Initialize player_out to None\n",
    "                remark_event = \"\"  # Initialize remark_event to empty string\n",
    "                player_assist = None  # Ensure this variable is also initialized\n",
    "\n",
    "                try:\n",
    "                    player_event_element = None\n",
    "                    full_text = item.find_element(By.XPATH, \".//div/div[4]\").text.strip()\n",
    "                    if event_type == \"Substitution\":\n",
    "                        parts = full_text.split('\\n')\n",
    "                        if len(parts) > 1:\n",
    "                            player_out_part = parts[-1]\n",
    "                            player_out_parts = player_out_part.split(', ')\n",
    "                            if len(player_out_parts) > 1:\n",
    "                                player_out = player_out_parts[0]\n",
    "                                remark_event = player_out_parts[1]\n",
    "                            else:\n",
    "                                player_out = player_out_parts[0]\n",
    "                        player_event_element = item.find_element(By.XPATH, \".//div/div[4]/span[1]/a\")\n",
    "                        player_event = player_event_element.get_attribute(\"title\")\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        player_event_element = item.find_element(By.XPATH, \".//div/div[4]/a\")\n",
    "                        player_event = player_event_element.get_attribute(\"title\")\n",
    "                        # Adjust this block to handle goals and cards specifically\n",
    "                        full_text = item.find_element(By.XPATH, \".//div/div[4]\").text\n",
    "                        if event_type == \"Goal\":\n",
    "                            parts = full_text.split(',')\n",
    "                            if len(parts) > 2:  # If there are at least 3 parts, indicating a remark is present\n",
    "                                remark_event = parts[1].strip()  # The part before the second ',' is the remark for goals\n",
    "                                # Handling Assist information for goals\n",
    "                                if \"Assist:\" in full_text:\n",
    "                                    assist_part = full_text.split('Assist:')[1].split(',')[0].strip()\n",
    "                                    player_assist = assist_part  # Assume player_assist is already defined elsewhere as None\n",
    "                            else:\n",
    "                                remark_event = parts[0].strip() if len(parts) > 1 else \"\"\n",
    "                        else:\n",
    "                            # For Cards, just an example, adjust as needed\n",
    "                            remark_event = full_text.split(',')[-1].strip() if ',' in full_text else full_text\n",
    "                except NoSuchElementException:\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "                card_type = event_type  # Default card type is the event type itself\n",
    "                if event_type == \"Card\":\n",
    "                    card_span_class = item.find_element(By.XPATH, \".//div/div[2]/span\").get_attribute(\"class\")\n",
    "                    if \"gelbrot\" in card_span_class:\n",
    "                        card_type = \"Yellow-Red Card\"\n",
    "                    elif \"gelb\" in card_span_class and \"rot\" not in card_span_class:\n",
    "                        card_type = \"Yellow Card\"\n",
    "                    elif \"rot\" in card_span_class:\n",
    "                        card_type = \"Direct Red Card\"\n",
    "\n",
    "                events_data.append({\n",
    "                    \"Timestamp\": timestamp,\n",
    "                    \"Club\": club,\n",
    "                    \"H/A\": team,\n",
    "                    \"Event\": card_type,\n",
    "                    \"Player Event\": player_event,\n",
    "                    \"Remark Event\": remark_event,\n",
    "                    \"Player Assist\": player_assist,\n",
    "                    \"Player Out\": player_out,\n",
    "                    \"Match ID\": match_id,\n",
    "                }) \n",
    "            return events_data\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No {event_type} events found on the page.\")\n",
    "            return []\n",
    "\n",
    "    all_events_data = []\n",
    "    event_types = {\"Goal\": '//*[@id=\"sb-tore\"]/ul', \"Substitution\": '//*[@id=\"sb-wechsel\"]/ul', \"Card\": '//*[@id=\"sb-karten\"]/ul'}\n",
    "\n",
    "    # Iterate through each event type and extract data\n",
    "    for event_type, xpath in event_types.items():\n",
    "        events_data = extract_events(xpath, event_type, home_club_name, away_club_name)\n",
    "        all_events_data.extend(events_data)\n",
    "\n",
    "    # Create DataFrame and reorder columns to put 'Timestamp' second\n",
    "    if all_events_data:  # Ensure there's data before creating the DataFrame\n",
    "        events_df = pd.DataFrame(all_events_data)\n",
    "        columns_order = ['Club', 'H/A', 'Timestamp', 'Event', 'Player Event', 'Remark Event', 'Player Assist', 'Player Out', 'Match ID']\n",
    "        events_df = events_df[columns_order]\n",
    "        all_events_dfs.append(events_df)\n",
    "    \n",
    "    print(f\"Scraping completed for match ID: {match_id}\")\n",
    "\n",
    "# Check if all_events_dfs is not empty before attempting to concatenate\n",
    "if all_events_dfs:  # This checks if the list is not empty\n",
    "    # Concatenate all events dataframes\n",
    "    final_events_df = pd.concat(all_events_dfs, ignore_index=True)\n",
    "\n",
    "    # Finally, save the dataframe to a CSV file for persistence\n",
    "    final_events_df.to_csv('match_events_2023_2024_1.csv', index=False)\n",
    "else:\n",
    "    print(\"No data was scraped.\")\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "final_events_df.to_csv('match_events_2023_2024_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4089693 # First Game ID of the season\n",
    "end_match_id = 4089824 # Adjust this according to your requirement\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "matches_info = []\n",
    "\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "\n",
    "    # Extract match information\n",
    "    try:\n",
    "        # Extract the home and away club names\n",
    "        league_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[1]/div/div[2]/h2/span/a').get_attribute(\"title\")\n",
    "        home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "        away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "        \n",
    "        # XPath for the result of the game\n",
    "        result_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/div/div/div')\n",
    "        result = result_element.text\n",
    "\n",
    "        # Append match information to the list\n",
    "        matches_info.append({\n",
    "            'Match ID': match_id,\n",
    "            'Home Team': home_club_name,\n",
    "            'Away Team': away_club_name,\n",
    "            'Result': result,\n",
    "            'League': league_name\n",
    "        })\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Match information not found for match ID: {match_id}\")\n",
    "\n",
    "# Convert the list of match information into a DataFrame\n",
    "matches_df = pd.DataFrame(matches_info)\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Match information successfully extracted.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "matches_df.to_csv('./data/matches_info_2022_2023_1.csv', index=False)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
