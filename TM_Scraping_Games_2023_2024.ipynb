{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "\n",
    "# Specify the path to the directory containing the ChromeDriver executable\n",
    "chrome_driver_directory = \"C:/Users/moren/Downloads/chromedriver-win64\" #insert your own path here #User moreno: 'moren'\n",
    "\n",
    "# Add the ChromeDriver directory to the PATH environment variable\n",
    "os.environ[\"PATH\"] += os.pathsep + chrome_driver_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1 dataframes after match ID: 4089710\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 2 dataframes after match ID: 4089711\n",
      "Concatenating 2 dataframes.\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4089710 # First Game ID of the season\n",
    "end_match_id = 4089711 # Adjust this according to your requirement\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store all lineup data\n",
    "lineups_df = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "all_lineup_stats_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 2)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "    \n",
    "    \n",
    "    ## SCRAPING ##\n",
    "    \n",
    "    # Extract the gameday information from the top of the page\n",
    "    #gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]/a[1]').text\n",
    "    gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]').text\n",
    "\n",
    "    # Extract the home and away club names from the 'title' attribute\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "\n",
    "\n",
    "    # Function to extract data from a table given its rows\n",
    "    def extract_table_data(table_rows, club_name):\n",
    "        positions = []\n",
    "        players = []\n",
    "        ages = []\n",
    "        market_values = []\n",
    "        club_names = [club_name] * (len(table_rows) // 3) # There's a club name for each player row\n",
    "        gamedays = [gameday] * (len(table_rows) // 3)  # Same gameday for all players in the match\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        for i in range(0, len(table_rows), 3):  # Increment by 3 for each player's data set\n",
    "            cells = table_rows[i].find_elements(By.TAG_NAME, \"td\")\n",
    "            player_info = cells[1].text\n",
    "            name_age_parts = player_info.split(' (')\n",
    "            player_name = name_age_parts[0].strip()\n",
    "            age_part = name_age_parts[1] if len(name_age_parts) > 1 else ''\n",
    "            age_match = re.search(r'(\\d+) years old', age_part)\n",
    "            age = age_match.group(1) if age_match else None\n",
    "            position_market_value = cells[4].text\n",
    "            if ', ' in position_market_value:\n",
    "                position, market_value = position_market_value.split(', ')\n",
    "            else:\n",
    "                position = position_market_value\n",
    "                market_value = None\n",
    "        \n",
    "            players.append(player_name)\n",
    "            ages.append(age)\n",
    "            positions.append(position)\n",
    "            market_values.append(market_value)\n",
    "    \n",
    "        return pd.DataFrame({\n",
    "            'Position': positions,\n",
    "            'Player': players,\n",
    "            'Age': ages,\n",
    "            'Market Value': market_values,\n",
    "            'Club': club_names,\n",
    "            'Gameday': gamedays,\n",
    "        })\n",
    "    all_tables_df = []\n",
    "\n",
    "    # XPath or CSS Selector for each table\n",
    "    tables_xpaths = {\n",
    "        'starting_lineup_home': '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[1]/table', \n",
    "        'substitutes_home': '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[1]/table',\n",
    "        'starting_lineup_away': '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[1]/table',\n",
    "        'substitutes_away': '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[1]/table'\n",
    "    }\n",
    "\n",
    "    all_tables_df = []\n",
    "\n",
    "    # Loop through the table paths and extract data\n",
    "    for key, xpath in tables_xpaths.items():\n",
    "        try:\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            team_type = 'Home' if 'home' in key else 'Away'\n",
    "            club_name = home_club_name if 'home' in key else away_club_name\n",
    "            df = extract_table_data(rows, club_name)  # Your custom function to extract data from rows\n",
    "            df['H/A'] = team_type\n",
    "            df['Status'] = 'Starting' if 'starting' in key else 'Substitute'\n",
    "            all_tables_df.append(df)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Table not found for {key} in match ID: {match_id}, skipping.\")\n",
    "            continue  # Skip this iteration if table is not found\n",
    "\n",
    "    # Combine all dataframes from the current page into lineups_df\n",
    "    if all_tables_df:  # Check if there's any data to concatenate\n",
    "        temp_df = pd.concat(all_tables_df, ignore_index=True)\n",
    "        temp_df['Match ID'] = match_id  # Add the match_id to every row in temp_df\n",
    "    \n",
    "        # Assuming lineups_df is defined somewhere above as the final dataframe\n",
    "        lineups_df = pd.concat([lineups_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Extract the home and away club names\n",
    "    home_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]')\n",
    "    home_club_name = home_club_name_element.get_attribute(\"title\")\n",
    "    away_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]')\n",
    "    away_club_name = away_club_name_element.get_attribute(\"title\")\n",
    "\n",
    "    # Extract home and away managers' names using the updated XPaths\n",
    "    home_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[1]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    home_manager_name = home_manager_element.text\n",
    "    away_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[2]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    away_manager_name = away_manager_element.text\n",
    "\n",
    "    # Extract additional information for both home and away teams with exception handling\n",
    "    try:\n",
    "        foreigners_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_away = \"N/A\"\n",
    "\n",
    "\n",
    "\n",
    "    # Function to clean the extracted data by removing preceding text\n",
    "    def clean_data(text, keep_eur_sign=False):\n",
    "        if keep_eur_sign:\n",
    "            # Directly slice away the preceding text if it follows a known pattern\n",
    "            if 'Purchase value: ' in text:\n",
    "                return text.replace('Purchase value: ', '')\n",
    "            elif 'Total MV: ' in text:\n",
    "                return text.replace('Total MV: ', '')\n",
    "        else:\n",
    "            # Using regex to find numeric values or percentages and return them for other columns\n",
    "            match = re.search(r'\\d+(\\.\\d+)?%?', text)\n",
    "            return match.group(0) if match else text\n",
    "    \n",
    "    # Create a DataFrame for the club and manager information along with the newly extracted data\n",
    "    lineups_stats_df = pd.DataFrame({\n",
    "        'Club': [home_club_name, away_club_name],\n",
    "        'H/A': ['Home', 'Away'],\n",
    "        'Manager': [home_manager_name, away_manager_name],\n",
    "        'Foreigners Starting': [clean_data(foreigners_starting_home), clean_data(foreigners_starting_away)],\n",
    "        'Foreigners Subs': [clean_data(foreigners_subs_home), clean_data(foreigners_subs_away)],\n",
    "        'Avg Age Starting': [clean_data(avg_age_starting_home), clean_data(avg_age_starting_away)],\n",
    "        'Avg Age Subs': [clean_data(avg_age_subs_home), clean_data(avg_age_subs_away)],\n",
    "        'Purchase Value Starting': [clean_data(purchase_value_starting_home, True), clean_data(purchase_value_starting_away, True)],\n",
    "        'Purchase Value Subs': [clean_data(purchase_value_subs_home, True), clean_data(purchase_value_subs_away, True)],\n",
    "        'Total Market Value Starting': [clean_data(total_market_value_starting_home, True), clean_data(total_market_value_starting_away, True)],\n",
    "        'Total Market Value Subs': [clean_data(total_market_value_subs_home, True), clean_data(total_market_value_subs_away, True)],\n",
    "        'Match ID': [match_id, match_id]\n",
    "    })\n",
    "\n",
    "    # Append the lineup stats dataframe for the current match to the list\n",
    "    all_lineup_stats_dfs.append(lineups_stats_df)\n",
    "\n",
    "    # Print the number of dataframes collected after each match\n",
    "    print(f\"Collected {len(all_lineup_stats_dfs)} dataframes after match ID: {match_id}\")\n",
    "\n",
    "\n",
    "# Before the concatenation, print out the number of dataframes to be concatenated\n",
    "print(f\"Concatenating {len(all_lineup_stats_dfs)} dataframes.\")\n",
    "\n",
    "# Concatenate all the lineup stats dataframes in the list\n",
    "final_lineup_stats_df = pd.concat(all_lineup_stats_dfs, ignore_index=True)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "lineups_df.to_csv('data/lineups_2023_2024_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1 dataframes after match ID: 4089693\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 2 dataframes after match ID: 4089694\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 3 dataframes after match ID: 4089695\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 4 dataframes after match ID: 4089696\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 5 dataframes after match ID: 4089697\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 6 dataframes after match ID: 4089698\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 7 dataframes after match ID: 4089699\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 8 dataframes after match ID: 4089700\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 9 dataframes after match ID: 4089701\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 10 dataframes after match ID: 4089702\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 11 dataframes after match ID: 4089703\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 12 dataframes after match ID: 4089704\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 13 dataframes after match ID: 4089705\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 14 dataframes after match ID: 4089706\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 15 dataframes after match ID: 4089707\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 16 dataframes after match ID: 4089708\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 17 dataframes after match ID: 4089709\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 18 dataframes after match ID: 4089710\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 19 dataframes after match ID: 4089711\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 20 dataframes after match ID: 4089712\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 21 dataframes after match ID: 4089713\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 22 dataframes after match ID: 4089714\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 23 dataframes after match ID: 4089715\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 24 dataframes after match ID: 4089716\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 25 dataframes after match ID: 4089717\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 26 dataframes after match ID: 4089718\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 27 dataframes after match ID: 4089719\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 28 dataframes after match ID: 4089720\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 29 dataframes after match ID: 4089721\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 30 dataframes after match ID: 4089722\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 31 dataframes after match ID: 4089723\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 32 dataframes after match ID: 4089724\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 33 dataframes after match ID: 4089725\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 34 dataframes after match ID: 4089726\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 35 dataframes after match ID: 4089727\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 36 dataframes after match ID: 4089728\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 37 dataframes after match ID: 4089729\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 38 dataframes after match ID: 4089730\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 39 dataframes after match ID: 4089731\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 40 dataframes after match ID: 4089732\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 41 dataframes after match ID: 4089733\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 42 dataframes after match ID: 4089734\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 43 dataframes after match ID: 4089735\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 44 dataframes after match ID: 4089736\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 45 dataframes after match ID: 4089737\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 46 dataframes after match ID: 4089738\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 47 dataframes after match ID: 4089739\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 48 dataframes after match ID: 4089740\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 49 dataframes after match ID: 4089741\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 50 dataframes after match ID: 4089742\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 51 dataframes after match ID: 4089743\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 52 dataframes after match ID: 4089744\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 53 dataframes after match ID: 4089745\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 54 dataframes after match ID: 4089746\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 55 dataframes after match ID: 4089747\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 56 dataframes after match ID: 4089748\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 57 dataframes after match ID: 4089749\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 58 dataframes after match ID: 4089750\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 59 dataframes after match ID: 4089751\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 60 dataframes after match ID: 4089752\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 61 dataframes after match ID: 4089753\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 62 dataframes after match ID: 4089754\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 63 dataframes after match ID: 4089755\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 64 dataframes after match ID: 4089756\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 65 dataframes after match ID: 4089757\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 66 dataframes after match ID: 4089758\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 67 dataframes after match ID: 4089759\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 68 dataframes after match ID: 4089760\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 69 dataframes after match ID: 4089761\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 70 dataframes after match ID: 4089762\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 71 dataframes after match ID: 4089763\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 72 dataframes after match ID: 4089764\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 73 dataframes after match ID: 4089765\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 74 dataframes after match ID: 4089766\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 75 dataframes after match ID: 4089767\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 76 dataframes after match ID: 4089768\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 77 dataframes after match ID: 4089769\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 78 dataframes after match ID: 4089770\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 79 dataframes after match ID: 4089771\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 80 dataframes after match ID: 4089772\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 81 dataframes after match ID: 4089773\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 82 dataframes after match ID: 4089774\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 83 dataframes after match ID: 4089775\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 84 dataframes after match ID: 4089776\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 85 dataframes after match ID: 4089777\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 86 dataframes after match ID: 4089778\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 87 dataframes after match ID: 4089779\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 88 dataframes after match ID: 4089780\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 89 dataframes after match ID: 4089781\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 90 dataframes after match ID: 4089782\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 91 dataframes after match ID: 4089783\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 92 dataframes after match ID: 4089784\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 93 dataframes after match ID: 4089785\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 94 dataframes after match ID: 4089786\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 95 dataframes after match ID: 4089787\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 96 dataframes after match ID: 4089788\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 97 dataframes after match ID: 4089789\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 98 dataframes after match ID: 4089790\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 99 dataframes after match ID: 4089791\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 100 dataframes after match ID: 4089792\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 101 dataframes after match ID: 4089793\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 102 dataframes after match ID: 4089794\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 103 dataframes after match ID: 4089795\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 104 dataframes after match ID: 4089796\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 105 dataframes after match ID: 4089797\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 106 dataframes after match ID: 4089798\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 107 dataframes after match ID: 4089799\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 108 dataframes after match ID: 4089800\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 109 dataframes after match ID: 4089801\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 110 dataframes after match ID: 4089802\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 111 dataframes after match ID: 4089803\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 112 dataframes after match ID: 4089804\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 113 dataframes after match ID: 4089805\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 114 dataframes after match ID: 4089806\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 115 dataframes after match ID: 4089807\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 116 dataframes after match ID: 4089808\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 117 dataframes after match ID: 4089809\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 118 dataframes after match ID: 4089810\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 119 dataframes after match ID: 4089811\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 120 dataframes after match ID: 4089812\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 121 dataframes after match ID: 4089813\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 122 dataframes after match ID: 4089814\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 123 dataframes after match ID: 4089815\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 124 dataframes after match ID: 4089816\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 125 dataframes after match ID: 4089817\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 126 dataframes after match ID: 4089818\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 127 dataframes after match ID: 4089819\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 128 dataframes after match ID: 4089820\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 129 dataframes after match ID: 4089821\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 130 dataframes after match ID: 4089822\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 131 dataframes after match ID: 4089823\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 132 dataframes after match ID: 4089824\n",
      "Concatenating 132 dataframes.\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4089693 # First Game ID of the season\n",
    "end_match_id = 4089824 # Adjust this according to your requirement\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store all lineup data\n",
    "lineups_df = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "all_lineup_stats_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "    \n",
    "    \n",
    "    ## SCRAPING ##\n",
    "    \n",
    "    # Extract the gameday information from the top of the page\n",
    "    #gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]/a[1]').text\n",
    "    gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]').text\n",
    "\n",
    "    # Extract the home and away club names from the 'title' attribute\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "\n",
    "\n",
    "    # Function to extract data from a table given its rows\n",
    "    def extract_table_data(table_rows, club_name):\n",
    "        positions = []\n",
    "        players = []\n",
    "        ages = []\n",
    "        market_values = []\n",
    "        club_names = [club_name] * (len(table_rows) // 3) # There's a club name for each player row\n",
    "        gamedays = [gameday] * (len(table_rows) // 3)  # Same gameday for all players in the match\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        for i in range(0, len(table_rows), 3):  # Increment by 3 for each player's data set\n",
    "            cells = table_rows[i].find_elements(By.TAG_NAME, \"td\")\n",
    "            player_info = cells[1].text\n",
    "            name_age_parts = player_info.split(' (')\n",
    "            player_name = name_age_parts[0].strip()\n",
    "            age_part = name_age_parts[1] if len(name_age_parts) > 1 else ''\n",
    "            age_match = re.search(r'(\\d+) years old', age_part)\n",
    "            age = age_match.group(1) if age_match else None\n",
    "            position_market_value = cells[4].text\n",
    "            if ', ' in position_market_value:\n",
    "                position, market_value = position_market_value.split(', ')\n",
    "            else:\n",
    "                position = position_market_value\n",
    "                market_value = None\n",
    "        \n",
    "            players.append(player_name)\n",
    "            ages.append(age)\n",
    "            positions.append(position)\n",
    "            market_values.append(market_value)\n",
    "    \n",
    "        return pd.DataFrame({\n",
    "            'Position': positions,\n",
    "            'Player': players,\n",
    "            'Age': ages,\n",
    "            'Market Value': market_values,\n",
    "            'Club': club_names,\n",
    "            'Gameday': gamedays,\n",
    "        })\n",
    "    all_tables_df = []\n",
    "\n",
    "    # XPath or CSS Selector for each table\n",
    "    tables_xpaths = {\n",
    "        'starting_lineup_home': '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[1]/table', \n",
    "        'substitutes_home': '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[1]/table',\n",
    "        'starting_lineup_away': '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[1]/table',\n",
    "        'substitutes_away': '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[1]/table'\n",
    "    }\n",
    "\n",
    "    all_tables_df = []\n",
    "\n",
    "    # Loop through the table paths and extract data\n",
    "    for key, xpath in tables_xpaths.items():\n",
    "        try:\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            team_type = 'Home' if 'home' in key else 'Away'\n",
    "            club_name = home_club_name if 'home' in key else away_club_name\n",
    "            df = extract_table_data(rows, club_name)  # Your custom function to extract data from rows\n",
    "            df['H/A'] = team_type\n",
    "            df['Status'] = 'Starting' if 'starting' in key else 'Substitute'\n",
    "            all_tables_df.append(df)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Table not found for {key} in match ID: {match_id}, skipping.\")\n",
    "            continue  # Skip this iteration if table is not found\n",
    "\n",
    "    # Combine all dataframes from the current page into lineups_df\n",
    "    if all_tables_df:  # Check if there's any data to concatenate\n",
    "        temp_df = pd.concat(all_tables_df, ignore_index=True)\n",
    "        temp_df['Match ID'] = match_id  # Add the match_id to every row in temp_df\n",
    "    \n",
    "        # Assuming lineups_df is defined somewhere above as the final dataframe\n",
    "        lineups_df = pd.concat([lineups_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Extract the home and away club names\n",
    "    home_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]')\n",
    "    home_club_name = home_club_name_element.get_attribute(\"title\")\n",
    "    away_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]')\n",
    "    away_club_name = away_club_name_element.get_attribute(\"title\")\n",
    "\n",
    "    # Extract home and away managers' names using the updated XPaths\n",
    "    home_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[1]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    home_manager_name = home_manager_element.text\n",
    "    away_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[2]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    away_manager_name = away_manager_element.text\n",
    "\n",
    "    # Extract additional information for both home and away teams with exception handling\n",
    "    try:\n",
    "        foreigners_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_away = \"N/A\"\n",
    "\n",
    "\n",
    "\n",
    "    # Function to clean the extracted data by removing preceding text\n",
    "    def clean_data(text, keep_eur_sign=False):\n",
    "        if keep_eur_sign:\n",
    "            # Directly slice away the preceding text if it follows a known pattern\n",
    "            if 'Purchase value: ' in text:\n",
    "                return text.replace('Purchase value: ', '')\n",
    "            elif 'Total MV: ' in text:\n",
    "                return text.replace('Total MV: ', '')\n",
    "        else:\n",
    "            # Using regex to find numeric values or percentages and return them for other columns\n",
    "            match = re.search(r'\\d+(\\.\\d+)?%?', text)\n",
    "            return match.group(0) if match else text\n",
    "    \n",
    "    # Create a DataFrame for the club and manager information along with the newly extracted data\n",
    "    lineups_stats_df = pd.DataFrame({\n",
    "        'Club': [home_club_name, away_club_name],\n",
    "        'H/A': ['Home', 'Away'],\n",
    "        'Manager': [home_manager_name, away_manager_name],\n",
    "        'Foreigners Starting': [clean_data(foreigners_starting_home), clean_data(foreigners_starting_away)],\n",
    "        'Foreigners Subs': [clean_data(foreigners_subs_home), clean_data(foreigners_subs_away)],\n",
    "        'Avg Age Starting': [clean_data(avg_age_starting_home), clean_data(avg_age_starting_away)],\n",
    "        'Avg Age Subs': [clean_data(avg_age_subs_home), clean_data(avg_age_subs_away)],\n",
    "        'Purchase Value Starting': [clean_data(purchase_value_starting_home, True), clean_data(purchase_value_starting_away, True)],\n",
    "        'Purchase Value Subs': [clean_data(purchase_value_subs_home, True), clean_data(purchase_value_subs_away, True)],\n",
    "        'Total Market Value Starting': [clean_data(total_market_value_starting_home, True), clean_data(total_market_value_starting_away, True)],\n",
    "        'Total Market Value Subs': [clean_data(total_market_value_subs_home, True), clean_data(total_market_value_subs_away, True)],\n",
    "        'Match ID': [match_id, match_id]\n",
    "    })\n",
    "\n",
    "    # Append the lineup stats dataframe for the current match to the list\n",
    "    all_lineup_stats_dfs.append(lineups_stats_df)\n",
    "\n",
    "    # Print the number of dataframes collected after each match\n",
    "    print(f\"Collected {len(all_lineup_stats_dfs)} dataframes after match ID: {match_id}\")\n",
    "\n",
    "\n",
    "# Before the concatenation, print out the number of dataframes to be concatenated\n",
    "print(f\"Concatenating {len(all_lineup_stats_dfs)} dataframes.\")\n",
    "\n",
    "# Concatenate all the lineup stats dataframes in the list\n",
    "final_lineup_stats_df = pd.concat(all_lineup_stats_dfs, ignore_index=True)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "lineups_df.to_csv('data/lineups_2023_2024_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1 dataframes after match ID: 4244785\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 2 dataframes after match ID: 4244786\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 3 dataframes after match ID: 4244787\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 4 dataframes after match ID: 4244788\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 5 dataframes after match ID: 4244789\n",
      "Concatenating 5 dataframes.\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4244785 # 4244791 First Game ID of the season\n",
    "end_match_id = 4244789 #4244838 # Adjust this according to your requirement\n",
    "\n",
    "# Initialize an empty DataFrame to store all lineup data\n",
    "lineups_df = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "all_lineup_stats_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "    \n",
    "    \n",
    "    ## SCRAPING ##\n",
    "    \n",
    "    # Extract the gameday information from the top of the page\n",
    "    #gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]/a[1]').text\n",
    "    gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]').text\n",
    "\n",
    "    # Extract the home and away club names from the 'title' attribute\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "\n",
    "\n",
    "    # Function to extract data from a table given its rows\n",
    "    def extract_table_data(table_rows, club_name):\n",
    "        positions = []\n",
    "        players = []\n",
    "        ages = []\n",
    "        market_values = []\n",
    "        club_names = [club_name] * (len(table_rows) // 3) # There's a club name for each player row\n",
    "        gamedays = [gameday] * (len(table_rows) // 3)  # Same gameday for all players in the match\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        for i in range(0, len(table_rows), 3):  # Increment by 3 for each player's data set\n",
    "            cells = table_rows[i].find_elements(By.TAG_NAME, \"td\")\n",
    "            player_info = cells[1].text\n",
    "            name_age_parts = player_info.split(' (')\n",
    "            player_name = name_age_parts[0].strip()\n",
    "            age_part = name_age_parts[1] if len(name_age_parts) > 1 else ''\n",
    "            age_match = re.search(r'(\\d+) years old', age_part)\n",
    "            age = age_match.group(1) if age_match else None\n",
    "            position_market_value = cells[4].text\n",
    "            if ', ' in position_market_value:\n",
    "                position, market_value = position_market_value.split(', ')\n",
    "            else:\n",
    "                position = position_market_value\n",
    "                market_value = None\n",
    "        \n",
    "            players.append(player_name)\n",
    "            ages.append(age)\n",
    "            positions.append(position)\n",
    "            market_values.append(market_value)\n",
    "    \n",
    "        return pd.DataFrame({\n",
    "            'Position': positions,\n",
    "            'Player': players,\n",
    "            'Age': ages,\n",
    "            'Market Value': market_values,\n",
    "            'Club': club_names,\n",
    "            'Gameday': gamedays,\n",
    "        })\n",
    "    all_tables_df = []\n",
    "\n",
    "    # XPath or CSS Selector for each table\n",
    "    tables_xpaths = {\n",
    "        'starting_lineup_home': '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[1]/table', \n",
    "        'substitutes_home': '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[1]/table',\n",
    "        'starting_lineup_away': '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[1]/table',\n",
    "        'substitutes_away': '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[1]/table'\n",
    "    }\n",
    "\n",
    "    all_tables_df = []\n",
    "\n",
    "    # Loop through the table paths and extract data\n",
    "    for key, xpath in tables_xpaths.items():\n",
    "        try:\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            team_type = 'Home' if 'home' in key else 'Away'\n",
    "            club_name = home_club_name if 'home' in key else away_club_name\n",
    "            df = extract_table_data(rows, club_name)  # Your custom function to extract data from rows\n",
    "            df['H/A'] = team_type\n",
    "            df['Status'] = 'Starting' if 'starting' in key else 'Substitute'\n",
    "            all_tables_df.append(df)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Table not found for {key} in match ID: {match_id}, skipping.\")\n",
    "            continue  # Skip this iteration if table is not found\n",
    "\n",
    "    # Combine all dataframes from the current page into lineups_df\n",
    "    if all_tables_df:  # Check if there's any data to concatenate\n",
    "        temp_df = pd.concat(all_tables_df, ignore_index=True)\n",
    "        temp_df['Match ID'] = match_id  # Add the match_id to every row in temp_df\n",
    "    \n",
    "        # Assuming lineups_df is defined somewhere above as the final dataframe\n",
    "        lineups_df = pd.concat([lineups_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Extract the home and away club names\n",
    "    home_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]')\n",
    "    home_club_name = home_club_name_element.get_attribute(\"title\")\n",
    "    away_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]')\n",
    "    away_club_name = away_club_name_element.get_attribute(\"title\")\n",
    "\n",
    "    # Extract home and away managers' names using the updated XPaths\n",
    "    home_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[1]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    home_manager_name = home_manager_element.text\n",
    "    away_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[2]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    away_manager_name = away_manager_element.text\n",
    "\n",
    "    # Extract additional information for both home and away teams with exception handling\n",
    "    try:\n",
    "        foreigners_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_away = \"N/A\"\n",
    "\n",
    "\n",
    "\n",
    "    # Function to clean the extracted data by removing preceding text\n",
    "    def clean_data(text, keep_eur_sign=False):\n",
    "        if keep_eur_sign:\n",
    "            # Directly slice away the preceding text if it follows a known pattern\n",
    "            if 'Purchase value: ' in text:\n",
    "                return text.replace('Purchase value: ', '')\n",
    "            elif 'Total MV: ' in text:\n",
    "                return text.replace('Total MV: ', '')\n",
    "        else:\n",
    "            # Using regex to find numeric values or percentages and return them for other columns\n",
    "            match = re.search(r'\\d+(\\.\\d+)?%?', text)\n",
    "            return match.group(0) if match else text\n",
    "    \n",
    "    # Create a DataFrame for the club and manager information along with the newly extracted data\n",
    "    lineups_stats_df = pd.DataFrame({\n",
    "        'Club': [home_club_name, away_club_name],\n",
    "        'H/A': ['Home', 'Away'],\n",
    "        'Manager': [home_manager_name, away_manager_name],\n",
    "        'Foreigners Starting': [clean_data(foreigners_starting_home), clean_data(foreigners_starting_away)],\n",
    "        'Foreigners Subs': [clean_data(foreigners_subs_home), clean_data(foreigners_subs_away)],\n",
    "        'Avg Age Starting': [clean_data(avg_age_starting_home), clean_data(avg_age_starting_away)],\n",
    "        'Avg Age Subs': [clean_data(avg_age_subs_home), clean_data(avg_age_subs_away)],\n",
    "        'Purchase Value Starting': [clean_data(purchase_value_starting_home, True), clean_data(purchase_value_starting_away, True)],\n",
    "        'Purchase Value Subs': [clean_data(purchase_value_subs_home, True), clean_data(purchase_value_subs_away, True)],\n",
    "        'Total Market Value Starting': [clean_data(total_market_value_starting_home, True), clean_data(total_market_value_starting_away, True)],\n",
    "        'Total Market Value Subs': [clean_data(total_market_value_subs_home, True), clean_data(total_market_value_subs_away, True)],\n",
    "        'Match ID': [match_id, match_id]\n",
    "    })\n",
    "\n",
    "    # Append the lineup stats dataframe for the current match to the list\n",
    "    all_lineup_stats_dfs.append(lineups_stats_df)\n",
    "\n",
    "    # Print the number of dataframes collected after each match\n",
    "    print(f\"Collected {len(all_lineup_stats_dfs)} dataframes after match ID: {match_id}\")\n",
    "\n",
    "\n",
    "# Before the concatenation, print out the number of dataframes to be concatenated\n",
    "print(f\"Concatenating {len(all_lineup_stats_dfs)} dataframes.\")\n",
    "\n",
    "# Concatenate all the lineup stats dataframes in the list\n",
    "final_lineup_stats_df = pd.concat(all_lineup_stats_dfs, ignore_index=True)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "lineups_df.to_csv('data/lineups_2023_2024_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1 dataframes after match ID: 4244791\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 2 dataframes after match ID: 4244792\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 3 dataframes after match ID: 4244793\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 4 dataframes after match ID: 4244794\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 5 dataframes after match ID: 4244795\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 6 dataframes after match ID: 4244796\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 7 dataframes after match ID: 4244797\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 8 dataframes after match ID: 4244798\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 9 dataframes after match ID: 4244799\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 10 dataframes after match ID: 4244800\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 11 dataframes after match ID: 4244801\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 12 dataframes after match ID: 4244802\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 13 dataframes after match ID: 4244803\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 14 dataframes after match ID: 4244804\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 15 dataframes after match ID: 4244805\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 16 dataframes after match ID: 4244806\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 17 dataframes after match ID: 4244807\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 18 dataframes after match ID: 4244808\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 19 dataframes after match ID: 4244809\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 20 dataframes after match ID: 4244810\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 21 dataframes after match ID: 4244811\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 22 dataframes after match ID: 4244812\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 23 dataframes after match ID: 4244813\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 24 dataframes after match ID: 4244814\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 25 dataframes after match ID: 4244815\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 26 dataframes after match ID: 4244816\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 27 dataframes after match ID: 4244817\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 28 dataframes after match ID: 4244818\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 29 dataframes after match ID: 4244819\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 30 dataframes after match ID: 4244820\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 31 dataframes after match ID: 4244821\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 32 dataframes after match ID: 4244822\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 33 dataframes after match ID: 4244823\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 34 dataframes after match ID: 4244824\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 35 dataframes after match ID: 4244825\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 36 dataframes after match ID: 4244826\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 37 dataframes after match ID: 4244827\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 38 dataframes after match ID: 4244828\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 39 dataframes after match ID: 4244829\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 40 dataframes after match ID: 4244830\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 41 dataframes after match ID: 4244831\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 42 dataframes after match ID: 4244832\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 43 dataframes after match ID: 4244833\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 44 dataframes after match ID: 4244834\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 45 dataframes after match ID: 4244835\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 46 dataframes after match ID: 4244836\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 47 dataframes after match ID: 4244837\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Collected 48 dataframes after match ID: 4244838\n",
      "Concatenating 48 dataframes.\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4244791 #First Game ID of the season\n",
    "end_match_id = 4244838 # Adjust this according to your requirement\n",
    "\n",
    "# Initialize an empty DataFrame to store all lineup data\n",
    "lineups_df = pd.DataFrame()\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "all_lineup_stats_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "    \n",
    "    \n",
    "    ## SCRAPING ##\n",
    "    \n",
    "    # Extract the gameday information from the top of the page\n",
    "    #gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]/a[1]').text\n",
    "    gameday = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/p[1]').text\n",
    "\n",
    "    # Extract the home and away club names from the 'title' attribute\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "\n",
    "\n",
    "    # Function to extract data from a table given its rows\n",
    "    def extract_table_data(table_rows, club_name):\n",
    "        positions = []\n",
    "        players = []\n",
    "        ages = []\n",
    "        market_values = []\n",
    "        club_names = [club_name] * (len(table_rows) // 3) # There's a club name for each player row\n",
    "        gamedays = [gameday] * (len(table_rows) // 3)  # Same gameday for all players in the match\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    \n",
    "        for i in range(0, len(table_rows), 3):  # Increment by 3 for each player's data set\n",
    "            cells = table_rows[i].find_elements(By.TAG_NAME, \"td\")\n",
    "            player_info = cells[1].text\n",
    "            name_age_parts = player_info.split(' (')\n",
    "            player_name = name_age_parts[0].strip()\n",
    "            age_part = name_age_parts[1] if len(name_age_parts) > 1 else ''\n",
    "            age_match = re.search(r'(\\d+) years old', age_part)\n",
    "            age = age_match.group(1) if age_match else None\n",
    "            position_market_value = cells[4].text\n",
    "            if ', ' in position_market_value:\n",
    "                position, market_value = position_market_value.split(', ')\n",
    "            else:\n",
    "                position = position_market_value\n",
    "                market_value = None\n",
    "        \n",
    "            players.append(player_name)\n",
    "            ages.append(age)\n",
    "            positions.append(position)\n",
    "            market_values.append(market_value)\n",
    "    \n",
    "        return pd.DataFrame({\n",
    "            'Position': positions,\n",
    "            'Player': players,\n",
    "            'Age': ages,\n",
    "            'Market Value': market_values,\n",
    "            'Club': club_names,\n",
    "            'Gameday': gamedays,\n",
    "        })\n",
    "    all_tables_df = []\n",
    "\n",
    "    # XPath or CSS Selector for each table\n",
    "    tables_xpaths = {\n",
    "        'starting_lineup_home': '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[1]/table', \n",
    "        'substitutes_home': '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[1]/table',\n",
    "        'starting_lineup_away': '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[1]/table',\n",
    "        'substitutes_away': '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[1]/table'\n",
    "    }\n",
    "\n",
    "    all_tables_df = []\n",
    "\n",
    "    # Loop through the table paths and extract data\n",
    "    for key, xpath in tables_xpaths.items():\n",
    "        try:\n",
    "            table = driver.find_element(By.XPATH, xpath)\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            team_type = 'Home' if 'home' in key else 'Away'\n",
    "            club_name = home_club_name if 'home' in key else away_club_name\n",
    "            df = extract_table_data(rows, club_name)  # Your custom function to extract data from rows\n",
    "            df['H/A'] = team_type\n",
    "            df['Status'] = 'Starting' if 'starting' in key else 'Substitute'\n",
    "            all_tables_df.append(df)\n",
    "        except NoSuchElementException:\n",
    "            print(f\"Table not found for {key} in match ID: {match_id}, skipping.\")\n",
    "            continue  # Skip this iteration if table is not found\n",
    "\n",
    "    # Combine all dataframes from the current page into lineups_df\n",
    "    if all_tables_df:  # Check if there's any data to concatenate\n",
    "        temp_df = pd.concat(all_tables_df, ignore_index=True)\n",
    "        temp_df['Match ID'] = match_id  # Add the match_id to every row in temp_df\n",
    "    \n",
    "        # Assuming lineups_df is defined somewhere above as the final dataframe\n",
    "        lineups_df = pd.concat([lineups_df, temp_df], ignore_index=True)\n",
    "\n",
    "    # Extract the home and away club names\n",
    "    home_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]')\n",
    "    home_club_name = home_club_name_element.get_attribute(\"title\")\n",
    "    away_club_name_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]')\n",
    "    away_club_name = away_club_name_element.get_attribute(\"title\")\n",
    "\n",
    "    # Extract home and away managers' names using the updated XPaths\n",
    "    home_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[1]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    home_manager_name = home_manager_element.text\n",
    "    away_manager_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[6]/div[2]/div/div/table/tbody/tr/td[1]/table/tbody/tr[1]/td[2]')\n",
    "    away_manager_name = away_manager_element.text\n",
    "\n",
    "    # Extract additional information for both home and away teams with exception handling\n",
    "    try:\n",
    "        foreigners_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_home = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[1]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_home = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        foreigners_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[1]').text\n",
    "    except NoSuchElementException:\n",
    "        foreigners_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        avg_age_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[2]').text\n",
    "    except NoSuchElementException:\n",
    "        avg_age_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        purchase_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[3]').text\n",
    "    except NoSuchElementException:\n",
    "        purchase_value_subs_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_starting_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[4]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_starting_away = \"N/A\"\n",
    "\n",
    "    try:\n",
    "        total_market_value_subs_away = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div[2]/div/div[2]/table/tbody/tr/td[4]').text\n",
    "    except NoSuchElementException:\n",
    "        total_market_value_subs_away = \"N/A\"\n",
    "\n",
    "\n",
    "\n",
    "    # Function to clean the extracted data by removing preceding text\n",
    "    def clean_data(text, keep_eur_sign=False):\n",
    "        if keep_eur_sign:\n",
    "            # Directly slice away the preceding text if it follows a known pattern\n",
    "            if 'Purchase value: ' in text:\n",
    "                return text.replace('Purchase value: ', '')\n",
    "            elif 'Total MV: ' in text:\n",
    "                return text.replace('Total MV: ', '')\n",
    "        else:\n",
    "            # Using regex to find numeric values or percentages and return them for other columns\n",
    "            match = re.search(r'\\d+(\\.\\d+)?%?', text)\n",
    "            return match.group(0) if match else text\n",
    "    \n",
    "    # Create a DataFrame for the club and manager information along with the newly extracted data\n",
    "    lineups_stats_df = pd.DataFrame({\n",
    "        'Club': [home_club_name, away_club_name],\n",
    "        'H/A': ['Home', 'Away'],\n",
    "        'Manager': [home_manager_name, away_manager_name],\n",
    "        'Foreigners Starting': [clean_data(foreigners_starting_home), clean_data(foreigners_starting_away)],\n",
    "        'Foreigners Subs': [clean_data(foreigners_subs_home), clean_data(foreigners_subs_away)],\n",
    "        'Avg Age Starting': [clean_data(avg_age_starting_home), clean_data(avg_age_starting_away)],\n",
    "        'Avg Age Subs': [clean_data(avg_age_subs_home), clean_data(avg_age_subs_away)],\n",
    "        'Purchase Value Starting': [clean_data(purchase_value_starting_home, True), clean_data(purchase_value_starting_away, True)],\n",
    "        'Purchase Value Subs': [clean_data(purchase_value_subs_home, True), clean_data(purchase_value_subs_away, True)],\n",
    "        'Total Market Value Starting': [clean_data(total_market_value_starting_home, True), clean_data(total_market_value_starting_away, True)],\n",
    "        'Total Market Value Subs': [clean_data(total_market_value_subs_home, True), clean_data(total_market_value_subs_away, True)],\n",
    "        'Match ID': [match_id, match_id]\n",
    "    })\n",
    "\n",
    "    # Append the lineup stats dataframe for the current match to the list\n",
    "    all_lineup_stats_dfs.append(lineups_stats_df)\n",
    "\n",
    "    # Print the number of dataframes collected after each match\n",
    "    print(f\"Collected {len(all_lineup_stats_dfs)} dataframes after match ID: {match_id}\")\n",
    "\n",
    "\n",
    "# Before the concatenation, print out the number of dataframes to be concatenated\n",
    "print(f\"Concatenating {len(all_lineup_stats_dfs)} dataframes.\")\n",
    "\n",
    "# Concatenate all the lineup stats dataframes in the list\n",
    "final_lineup_stats_df = pd.concat(all_lineup_stats_dfs, ignore_index=True)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "lineups_df.to_csv('data/lineups_2023_2024_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineups_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_lineup_stats_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_lineup_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lineups_stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed for match ID: 4089693\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089694\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Goal events found on the page.\n",
      "Scraping completed for match ID: 4089695\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089696\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089697\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089698\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089699\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089700\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089701\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089702\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089703\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089704\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089705\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089706\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089707\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089708\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Card events found on the page.\n",
      "Scraping completed for match ID: 4089709\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089710\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089711\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089712\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089713\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089714\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089715\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089716\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089717\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089718\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089719\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089720\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089721\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089722\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089723\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089724\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089725\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089726\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089727\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089728\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Goal events found on the page.\n",
      "Scraping completed for match ID: 4089729\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089730\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089731\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089732\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089733\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089734\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089735\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089736\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089737\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089738\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089739\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089740\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089741\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089742\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089743\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089744\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089745\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089746\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089747\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089748\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089749\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089750\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089751\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089752\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089753\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Goal events found on the page.\n",
      "Scraping completed for match ID: 4089754\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089755\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089756\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089757\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089758\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089759\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089760\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089761\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089762\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089763\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089764\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089765\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089766\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089767\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089768\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089769\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089770\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089771\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Card events found on the page.\n",
      "Scraping completed for match ID: 4089772\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089773\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089774\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089775\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089776\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089777\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089778\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089779\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089780\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089781\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089782\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089783\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089784\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089785\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089786\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089787\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089788\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089789\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089790\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089791\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Card events found on the page.\n",
      "Scraping completed for match ID: 4089792\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089793\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089794\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089795\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089796\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089797\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089798\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089799\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089800\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089801\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089802\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089803\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089804\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089805\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Goal events found on the page.\n",
      "Scraping completed for match ID: 4089806\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089807\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089808\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089809\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089810\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089811\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089812\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Goal events found on the page.\n",
      "Scraping completed for match ID: 4089813\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089814\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089815\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089816\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089817\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089818\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089819\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089820\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089821\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089822\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089823\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4089824\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4089693  # First Game ID of the season\n",
    "end_match_id = 4089824 #4244838  # Adjust this according to your requirement\n",
    "\n",
    "# Initialize an empty list to store all events dataframes\n",
    "all_events_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/index/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Handling the iframe and accept button if exists\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 2)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "        driver.switch_to.default_content()\n",
    "    except:\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "\n",
    "    ## SCRAPING ## \n",
    "\n",
    "    # Extracting club names\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div/div/div[1]/div[1]/div[2]/nobr/a').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div/div/div[2]/div[1]/div[2]/nobr/a').get_attribute(\"title\")\n",
    "\n",
    "    # Function to convert pixel values to minutes based on the pattern provided\n",
    "    def convert_px_to_minute(x_px, y_px):\n",
    "        # Remove any non-numeric characters and convert to integer\n",
    "        x_px = int(re.sub(r'[^\\d-]', '', str(x_px)))\n",
    "        y_px = int(re.sub(r'[^\\d-]', '', str(y_px)))\n",
    "    \n",
    "        # Convert negative values to positive\n",
    "        x_px = abs(x_px)\n",
    "        y_px = abs(y_px)\n",
    "    \n",
    "        unit_minutes = (x_px // 36) + 1\n",
    "        ten_minutes = (y_px // 36) * 10\n",
    "        timestamp = f\"{unit_minutes + ten_minutes}'\"\n",
    "        return timestamp\n",
    "\n",
    "\n",
    "    def extract_px_from_style(style_str):\n",
    "        # Use regular expression to find all pixel values in the style string\n",
    "        px_values = re.findall(r'-?\\d+px', style_str)  # Include optional minus sign\n",
    "    \n",
    "        # Check if there are at least two pixel values\n",
    "        if len(px_values) >= 2:\n",
    "            x_px, y_px = [int(px.strip('px')) for px in px_values[:2]]  # Take the first two values\n",
    "            return x_px, y_px\n",
    "        else:\n",
    "            # Handle the case when there are not enough values\n",
    "            return None, None  # You can return None or some default values\n",
    "\n",
    "\n",
    "    # Function to extract events with Remark Event adjustment\n",
    "    def extract_events(event_type_xpath, event_type, home_club_name, away_club_name):\n",
    "        try:\n",
    "            events_list = driver.find_element(By.XPATH, event_type_xpath)\n",
    "            events_items = events_list.find_elements(By.TAG_NAME, \"li\")\n",
    "            events_data = []\n",
    "\n",
    "            for item in events_items:\n",
    "                team = \"Home\" if \"heim\" in item.get_attribute(\"class\") else \"Away\"\n",
    "                club = home_club_name if team == \"Home\" else away_club_name\n",
    "\n",
    "                # Extract the style attribute for timestamp\n",
    "                style_str = item.find_element(By.XPATH, \".//div/div[1]/span\").get_attribute(\"style\")\n",
    "                x_px, y_px = extract_px_from_style(style_str)\n",
    "                timestamp = convert_px_to_minute(x_px, y_px)\n",
    "\n",
    "                player_event = \"N/A\"  # Default value if player name is not found\n",
    "                player_out = None  # Initialize player_out to None\n",
    "                remark_event = \"\"  # Initialize remark_event to empty string\n",
    "                player_assist = None  # Ensure this variable is also initialized\n",
    "\n",
    "                try:\n",
    "                    player_event_element = None\n",
    "                    full_text = item.find_element(By.XPATH, \".//div/div[4]\").text.strip()\n",
    "                    if event_type == \"Substitution\":\n",
    "                        parts = full_text.split('\\n')\n",
    "                        if len(parts) > 1:\n",
    "                            player_out_part = parts[-1]\n",
    "                            player_out_parts = player_out_part.split(', ')\n",
    "                            if len(player_out_parts) > 1:\n",
    "                                player_out = player_out_parts[0]\n",
    "                                remark_event = player_out_parts[1]\n",
    "                            else:\n",
    "                                player_out = player_out_parts[0]\n",
    "                        player_event_element = item.find_element(By.XPATH, \".//div/div[4]/span[1]/a\")\n",
    "                        player_event = player_event_element.get_attribute(\"title\")\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        player_event_element = item.find_element(By.XPATH, \".//div/div[4]/a\")\n",
    "                        player_event = player_event_element.get_attribute(\"title\")\n",
    "                        # Adjust this block to handle goals and cards specifically\n",
    "                        full_text = item.find_element(By.XPATH, \".//div/div[4]\").text\n",
    "                        if event_type == \"Goal\":\n",
    "                            parts = full_text.split(',')\n",
    "                            if len(parts) > 2:  # If there are at least 3 parts, indicating a remark is present\n",
    "                                remark_event = parts[1].strip()  # The part before the second ',' is the remark for goals\n",
    "                                # Handling Assist information for goals\n",
    "                                if \"Assist:\" in full_text:\n",
    "                                    assist_part = full_text.split('Assist:')[1].split(',')[0].strip()\n",
    "                                    player_assist = assist_part  # Assume player_assist is already defined elsewhere as None\n",
    "                            else:\n",
    "                                remark_event = parts[0].strip() if len(parts) > 1 else \"\"\n",
    "                        else:\n",
    "                            # For Cards, just an example, adjust as needed\n",
    "                            remark_event = full_text.split(',')[-1].strip() if ',' in full_text else full_text\n",
    "                except NoSuchElementException:\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "                card_type = event_type  # Default card type is the event type itself\n",
    "                if event_type == \"Card\":\n",
    "                    card_span_class = item.find_element(By.XPATH, \".//div/div[2]/span\").get_attribute(\"class\")\n",
    "                    if \"gelbrot\" in card_span_class:\n",
    "                        card_type = \"Yellow-Red Card\"\n",
    "                    elif \"gelb\" in card_span_class and \"rot\" not in card_span_class:\n",
    "                        card_type = \"Yellow Card\"\n",
    "                    elif \"rot\" in card_span_class:\n",
    "                        card_type = \"Direct Red Card\"\n",
    "\n",
    "                events_data.append({\n",
    "                    \"Timestamp\": timestamp,\n",
    "                    \"Club\": club,\n",
    "                    \"H/A\": team,\n",
    "                    \"Event\": card_type,\n",
    "                    \"Player Event\": player_event,\n",
    "                    \"Remark Event\": remark_event,\n",
    "                    \"Player Assist\": player_assist,\n",
    "                    \"Player Out\": player_out,\n",
    "                    \"Match ID\": match_id,\n",
    "                }) \n",
    "            return events_data\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No {event_type} events found on the page.\")\n",
    "            return []\n",
    "\n",
    "    all_events_data = []\n",
    "    event_types = {\"Goal\": '//*[@id=\"sb-tore\"]/ul', \"Substitution\": '//*[@id=\"sb-wechsel\"]/ul', \"Card\": '//*[@id=\"sb-karten\"]/ul'}\n",
    "\n",
    "    # Iterate through each event type and extract data\n",
    "    for event_type, xpath in event_types.items():\n",
    "        events_data = extract_events(xpath, event_type, home_club_name, away_club_name)\n",
    "        all_events_data.extend(events_data)\n",
    "\n",
    "    # Create DataFrame and reorder columns to put 'Timestamp' second\n",
    "    if all_events_data:  # Ensure there's data before creating the DataFrame\n",
    "        events_df = pd.DataFrame(all_events_data)\n",
    "        columns_order = ['Club', 'H/A', 'Timestamp', 'Event', 'Player Event', 'Remark Event', 'Player Assist', 'Player Out', 'Match ID']\n",
    "        events_df = events_df[columns_order]\n",
    "        all_events_dfs.append(events_df)\n",
    "    \n",
    "    print(f\"Scraping completed for match ID: {match_id}\")\n",
    "\n",
    "# Check if all_events_dfs is not empty before attempting to concatenate\n",
    "if all_events_dfs:  # This checks if the list is not empty\n",
    "    # Concatenate all events dataframes\n",
    "    final_events_df = pd.concat(all_events_dfs, ignore_index=True)\n",
    "\n",
    "    # Finally, save the dataframe to a CSV file for persistence\n",
    "    final_events_df.to_csv('data/match_events_2023_2024_1.csv', index=False)\n",
    "else:\n",
    "    print(\"No data was scraped.\")\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "final_events_df.to_csv('data/match_events_2023_2024_1.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed for match ID: 4244785\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244786\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244787\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244788\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244789\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4244785 # 4244791 First Game ID of the season\n",
    "end_match_id = 4244789 #4244838 # Adjust this according to your requirement\n",
    "\n",
    "# Initialize an empty list to store all events dataframes\n",
    "all_events_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/index/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Handling the iframe and accept button if exists\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 2)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "        driver.switch_to.default_content()\n",
    "    except:\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "\n",
    "    ## SCRAPING ## \n",
    "\n",
    "    # Extracting club names\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div/div/div[1]/div[1]/div[2]/nobr/a').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div/div/div[2]/div[1]/div[2]/nobr/a').get_attribute(\"title\")\n",
    "\n",
    "    # Function to convert pixel values to minutes based on the pattern provided\n",
    "    def convert_px_to_minute(x_px, y_px):\n",
    "        # Remove any non-numeric characters and convert to integer\n",
    "        x_px = int(re.sub(r'[^\\d-]', '', str(x_px)))\n",
    "        y_px = int(re.sub(r'[^\\d-]', '', str(y_px)))\n",
    "    \n",
    "        # Convert negative values to positive\n",
    "        x_px = abs(x_px)\n",
    "        y_px = abs(y_px)\n",
    "    \n",
    "        unit_minutes = (x_px // 36) + 1\n",
    "        ten_minutes = (y_px // 36) * 10\n",
    "        timestamp = f\"{unit_minutes + ten_minutes}'\"\n",
    "        return timestamp\n",
    "\n",
    "\n",
    "    def extract_px_from_style(style_str):\n",
    "        # Use regular expression to find all pixel values in the style string\n",
    "        px_values = re.findall(r'-?\\d+px', style_str)  # Include optional minus sign\n",
    "    \n",
    "        # Check if there are at least two pixel values\n",
    "        if len(px_values) >= 2:\n",
    "            x_px, y_px = [int(px.strip('px')) for px in px_values[:2]]  # Take the first two values\n",
    "            return x_px, y_px\n",
    "        else:\n",
    "            # Handle the case when there are not enough values\n",
    "            return None, None  # You can return None or some default values\n",
    "\n",
    "\n",
    "    # Function to extract events with Remark Event adjustment\n",
    "    def extract_events(event_type_xpath, event_type, home_club_name, away_club_name):\n",
    "        try:\n",
    "            events_list = driver.find_element(By.XPATH, event_type_xpath)\n",
    "            events_items = events_list.find_elements(By.TAG_NAME, \"li\")\n",
    "            events_data = []\n",
    "\n",
    "            for item in events_items:\n",
    "                team = \"Home\" if \"heim\" in item.get_attribute(\"class\") else \"Away\"\n",
    "                club = home_club_name if team == \"Home\" else away_club_name\n",
    "\n",
    "                # Extract the style attribute for timestamp\n",
    "                style_str = item.find_element(By.XPATH, \".//div/div[1]/span\").get_attribute(\"style\")\n",
    "                x_px, y_px = extract_px_from_style(style_str)\n",
    "                timestamp = convert_px_to_minute(x_px, y_px)\n",
    "\n",
    "                player_event = \"N/A\"  # Default value if player name is not found\n",
    "                player_out = None  # Initialize player_out to None\n",
    "                remark_event = \"\"  # Initialize remark_event to empty string\n",
    "                player_assist = None  # Ensure this variable is also initialized\n",
    "\n",
    "                try:\n",
    "                    player_event_element = None\n",
    "                    full_text = item.find_element(By.XPATH, \".//div/div[4]\").text.strip()\n",
    "                    if event_type == \"Substitution\":\n",
    "                        parts = full_text.split('\\n')\n",
    "                        if len(parts) > 1:\n",
    "                            player_out_part = parts[-1]\n",
    "                            player_out_parts = player_out_part.split(', ')\n",
    "                            if len(player_out_parts) > 1:\n",
    "                                player_out = player_out_parts[0]\n",
    "                                remark_event = player_out_parts[1]\n",
    "                            else:\n",
    "                                player_out = player_out_parts[0]\n",
    "                        player_event_element = item.find_element(By.XPATH, \".//div/div[4]/span[1]/a\")\n",
    "                        player_event = player_event_element.get_attribute(\"title\")\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        player_event_element = item.find_element(By.XPATH, \".//div/div[4]/a\")\n",
    "                        player_event = player_event_element.get_attribute(\"title\")\n",
    "                        # Adjust this block to handle goals and cards specifically\n",
    "                        full_text = item.find_element(By.XPATH, \".//div/div[4]\").text\n",
    "                        if event_type == \"Goal\":\n",
    "                            parts = full_text.split(',')\n",
    "                            if len(parts) > 2:  # If there are at least 3 parts, indicating a remark is present\n",
    "                                remark_event = parts[1].strip()  # The part before the second ',' is the remark for goals\n",
    "                                # Handling Assist information for goals\n",
    "                                if \"Assist:\" in full_text:\n",
    "                                    assist_part = full_text.split('Assist:')[1].split(',')[0].strip()\n",
    "                                    player_assist = assist_part  # Assume player_assist is already defined elsewhere as None\n",
    "                            else:\n",
    "                                remark_event = parts[0].strip() if len(parts) > 1 else \"\"\n",
    "                        else:\n",
    "                            # For Cards, just an example, adjust as needed\n",
    "                            remark_event = full_text.split(',')[-1].strip() if ',' in full_text else full_text\n",
    "                except NoSuchElementException:\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "                card_type = event_type  # Default card type is the event type itself\n",
    "                if event_type == \"Card\":\n",
    "                    card_span_class = item.find_element(By.XPATH, \".//div/div[2]/span\").get_attribute(\"class\")\n",
    "                    if \"gelbrot\" in card_span_class:\n",
    "                        card_type = \"Yellow-Red Card\"\n",
    "                    elif \"gelb\" in card_span_class and \"rot\" not in card_span_class:\n",
    "                        card_type = \"Yellow Card\"\n",
    "                    elif \"rot\" in card_span_class:\n",
    "                        card_type = \"Direct Red Card\"\n",
    "\n",
    "                events_data.append({\n",
    "                    \"Timestamp\": timestamp,\n",
    "                    \"Club\": club,\n",
    "                    \"H/A\": team,\n",
    "                    \"Event\": card_type,\n",
    "                    \"Player Event\": player_event,\n",
    "                    \"Remark Event\": remark_event,\n",
    "                    \"Player Assist\": player_assist,\n",
    "                    \"Player Out\": player_out,\n",
    "                    \"Match ID\": match_id,\n",
    "                }) \n",
    "            return events_data\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No {event_type} events found on the page.\")\n",
    "            return []\n",
    "\n",
    "    all_events_data = []\n",
    "    event_types = {\"Goal\": '//*[@id=\"sb-tore\"]/ul', \"Substitution\": '//*[@id=\"sb-wechsel\"]/ul', \"Card\": '//*[@id=\"sb-karten\"]/ul'}\n",
    "\n",
    "    # Iterate through each event type and extract data\n",
    "    for event_type, xpath in event_types.items():\n",
    "        events_data = extract_events(xpath, event_type, home_club_name, away_club_name)\n",
    "        all_events_data.extend(events_data)\n",
    "\n",
    "    # Create DataFrame and reorder columns to put 'Timestamp' second\n",
    "    if all_events_data:  # Ensure there's data before creating the DataFrame\n",
    "        events_df = pd.DataFrame(all_events_data)\n",
    "        columns_order = ['Club', 'H/A', 'Timestamp', 'Event', 'Player Event', 'Remark Event', 'Player Assist', 'Player Out', 'Match ID']\n",
    "        events_df = events_df[columns_order]\n",
    "        all_events_dfs.append(events_df)\n",
    "    \n",
    "    print(f\"Scraping completed for match ID: {match_id}\")\n",
    "\n",
    "# Check if all_events_dfs is not empty before attempting to concatenate\n",
    "if all_events_dfs:  # This checks if the list is not empty\n",
    "    # Concatenate all events dataframes\n",
    "    final_events_df = pd.concat(all_events_dfs, ignore_index=True)\n",
    "\n",
    "    # Finally, save the dataframe to a CSV file for persistence\n",
    "    final_events_df.to_csv('data/match_events_2023_2024_2.csv', index=False)\n",
    "else:\n",
    "    print(\"No data was scraped.\")\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "final_events_df.to_csv('data/match_events_2023_2024_2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed for match ID: 4244791\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244792\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244793\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244794\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244795\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244796\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244797\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244798\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244799\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244800\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244801\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244802\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244803\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244804\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244805\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244806\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244807\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244808\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244809\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244810\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244811\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244812\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244813\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244814\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244815\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244816\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244817\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244818\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244819\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244820\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244821\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244822\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244823\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Goal events found on the page.\n",
      "Scraping completed for match ID: 4244824\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244825\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244826\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244827\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244828\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Goal events found on the page.\n",
      "Scraping completed for match ID: 4244829\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244830\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244831\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Card events found on the page.\n",
      "Scraping completed for match ID: 4244832\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244833\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244834\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244835\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "No Goal events found on the page.\n",
      "Scraping completed for match ID: 4244836\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244837\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Scraping completed for match ID: 4244838\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4244791 # 4244791 First Game ID of the season\n",
    "end_match_id = 4244838 #4244838 # Adjust this according to your requirement\n",
    "\n",
    "# Initialize an empty list to store all events dataframes\n",
    "all_events_dfs = []\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/index/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Handling the iframe and accept button if exists\n",
    "    try:\n",
    "        wait = WebDriverWait(driver, 2)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "        driver.switch_to.default_content()\n",
    "    except:\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "\n",
    "    ## SCRAPING ## \n",
    "\n",
    "    # Extracting club names\n",
    "    home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div/div/div[1]/div[1]/div[2]/nobr/a').get_attribute(\"title\")\n",
    "    away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[5]/div/div/div[2]/div[1]/div[2]/nobr/a').get_attribute(\"title\")\n",
    "\n",
    "    # Function to convert pixel values to minutes based on the pattern provided\n",
    "    def convert_px_to_minute(x_px, y_px):\n",
    "        # Remove any non-numeric characters and convert to integer\n",
    "        x_px = int(re.sub(r'[^\\d-]', '', str(x_px)))\n",
    "        y_px = int(re.sub(r'[^\\d-]', '', str(y_px)))\n",
    "    \n",
    "        # Convert negative values to positive\n",
    "        x_px = abs(x_px)\n",
    "        y_px = abs(y_px)\n",
    "    \n",
    "        unit_minutes = (x_px // 36) + 1\n",
    "        ten_minutes = (y_px // 36) * 10\n",
    "        timestamp = f\"{unit_minutes + ten_minutes}'\"\n",
    "        return timestamp\n",
    "\n",
    "\n",
    "    def extract_px_from_style(style_str):\n",
    "        # Use regular expression to find all pixel values in the style string\n",
    "        px_values = re.findall(r'-?\\d+px', style_str)  # Include optional minus sign\n",
    "    \n",
    "        # Check if there are at least two pixel values\n",
    "        if len(px_values) >= 2:\n",
    "            x_px, y_px = [int(px.strip('px')) for px in px_values[:2]]  # Take the first two values\n",
    "            return x_px, y_px\n",
    "        else:\n",
    "            # Handle the case when there are not enough values\n",
    "            return None, None  # You can return None or some default values\n",
    "\n",
    "\n",
    "    # Function to extract events with Remark Event adjustment\n",
    "    def extract_events(event_type_xpath, event_type, home_club_name, away_club_name):\n",
    "        try:\n",
    "            events_list = driver.find_element(By.XPATH, event_type_xpath)\n",
    "            events_items = events_list.find_elements(By.TAG_NAME, \"li\")\n",
    "            events_data = []\n",
    "\n",
    "            for item in events_items:\n",
    "                team = \"Home\" if \"heim\" in item.get_attribute(\"class\") else \"Away\"\n",
    "                club = home_club_name if team == \"Home\" else away_club_name\n",
    "\n",
    "                # Extract the style attribute for timestamp\n",
    "                style_str = item.find_element(By.XPATH, \".//div/div[1]/span\").get_attribute(\"style\")\n",
    "                x_px, y_px = extract_px_from_style(style_str)\n",
    "                timestamp = convert_px_to_minute(x_px, y_px)\n",
    "\n",
    "                player_event = \"N/A\"  # Default value if player name is not found\n",
    "                player_out = None  # Initialize player_out to None\n",
    "                remark_event = \"\"  # Initialize remark_event to empty string\n",
    "                player_assist = None  # Ensure this variable is also initialized\n",
    "\n",
    "                try:\n",
    "                    player_event_element = None\n",
    "                    full_text = item.find_element(By.XPATH, \".//div/div[4]\").text.strip()\n",
    "                    if event_type == \"Substitution\":\n",
    "                        parts = full_text.split('\\n')\n",
    "                        if len(parts) > 1:\n",
    "                            player_out_part = parts[-1]\n",
    "                            player_out_parts = player_out_part.split(', ')\n",
    "                            if len(player_out_parts) > 1:\n",
    "                                player_out = player_out_parts[0]\n",
    "                                remark_event = player_out_parts[1]\n",
    "                            else:\n",
    "                                player_out = player_out_parts[0]\n",
    "                        player_event_element = item.find_element(By.XPATH, \".//div/div[4]/span[1]/a\")\n",
    "                        player_event = player_event_element.get_attribute(\"title\")\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        player_event_element = item.find_element(By.XPATH, \".//div/div[4]/a\")\n",
    "                        player_event = player_event_element.get_attribute(\"title\")\n",
    "                        # Adjust this block to handle goals and cards specifically\n",
    "                        full_text = item.find_element(By.XPATH, \".//div/div[4]\").text\n",
    "                        if event_type == \"Goal\":\n",
    "                            parts = full_text.split(',')\n",
    "                            if len(parts) > 2:  # If there are at least 3 parts, indicating a remark is present\n",
    "                                remark_event = parts[1].strip()  # The part before the second ',' is the remark for goals\n",
    "                                # Handling Assist information for goals\n",
    "                                if \"Assist:\" in full_text:\n",
    "                                    assist_part = full_text.split('Assist:')[1].split(',')[0].strip()\n",
    "                                    player_assist = assist_part  # Assume player_assist is already defined elsewhere as None\n",
    "                            else:\n",
    "                                remark_event = parts[0].strip() if len(parts) > 1 else \"\"\n",
    "                        else:\n",
    "                            # For Cards, just an example, adjust as needed\n",
    "                            remark_event = full_text.split(',')[-1].strip() if ',' in full_text else full_text\n",
    "                except NoSuchElementException:\n",
    "                    pass\n",
    "\n",
    "\n",
    "\n",
    "                card_type = event_type  # Default card type is the event type itself\n",
    "                if event_type == \"Card\":\n",
    "                    card_span_class = item.find_element(By.XPATH, \".//div/div[2]/span\").get_attribute(\"class\")\n",
    "                    if \"gelbrot\" in card_span_class:\n",
    "                        card_type = \"Yellow-Red Card\"\n",
    "                    elif \"gelb\" in card_span_class and \"rot\" not in card_span_class:\n",
    "                        card_type = \"Yellow Card\"\n",
    "                    elif \"rot\" in card_span_class:\n",
    "                        card_type = \"Direct Red Card\"\n",
    "\n",
    "                events_data.append({\n",
    "                    \"Timestamp\": timestamp,\n",
    "                    \"Club\": club,\n",
    "                    \"H/A\": team,\n",
    "                    \"Event\": card_type,\n",
    "                    \"Player Event\": player_event,\n",
    "                    \"Remark Event\": remark_event,\n",
    "                    \"Player Assist\": player_assist,\n",
    "                    \"Player Out\": player_out,\n",
    "                    \"Match ID\": match_id,\n",
    "                }) \n",
    "            return events_data\n",
    "        except NoSuchElementException:\n",
    "            print(f\"No {event_type} events found on the page.\")\n",
    "            return []\n",
    "\n",
    "    all_events_data = []\n",
    "    event_types = {\"Goal\": '//*[@id=\"sb-tore\"]/ul', \"Substitution\": '//*[@id=\"sb-wechsel\"]/ul', \"Card\": '//*[@id=\"sb-karten\"]/ul'}\n",
    "\n",
    "    # Iterate through each event type and extract data\n",
    "    for event_type, xpath in event_types.items():\n",
    "        events_data = extract_events(xpath, event_type, home_club_name, away_club_name)\n",
    "        all_events_data.extend(events_data)\n",
    "\n",
    "    # Create DataFrame and reorder columns to put 'Timestamp' second\n",
    "    if all_events_data:  # Ensure there's data before creating the DataFrame\n",
    "        events_df = pd.DataFrame(all_events_data)\n",
    "        columns_order = ['Club', 'H/A', 'Timestamp', 'Event', 'Player Event', 'Remark Event', 'Player Assist', 'Player Out', 'Match ID']\n",
    "        events_df = events_df[columns_order]\n",
    "        all_events_dfs.append(events_df)\n",
    "    \n",
    "    print(f\"Scraping completed for match ID: {match_id}\")\n",
    "\n",
    "# Check if all_events_dfs is not empty before attempting to concatenate\n",
    "if all_events_dfs:  # This checks if the list is not empty\n",
    "    # Concatenate all events dataframes\n",
    "    final_events_df = pd.concat(all_events_dfs, ignore_index=True)\n",
    "\n",
    "    # Finally, save the dataframe to a CSV file for persistence\n",
    "    final_events_df.to_csv('data/match_events_2023_2024_3.csv', index=False)\n",
    "else:\n",
    "    print(\"No data was scraped.\")\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "final_events_df.to_csv('data/match_events_2023_2024_3.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_events_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Match information successfully extracted.\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4089693 # First Game ID of the season\n",
    "end_match_id = 4089824 # Adjust this according to your requirement\n",
    "\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "matches_info = []\n",
    "\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "\n",
    "    # Extract match information\n",
    "    try:\n",
    "        # Extract the home and away club names\n",
    "        league_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[1]/div/div[2]/h2/span/a').get_attribute(\"title\")\n",
    "        home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "        away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "        \n",
    "        # XPath for the result of the game\n",
    "        result_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/div/div/div')\n",
    "        result = result_element.text\n",
    "\n",
    "        # Append match information to the list\n",
    "        matches_info.append({\n",
    "            'Match ID': match_id,\n",
    "            'Home Team': home_club_name,\n",
    "            'Away Team': away_club_name,\n",
    "            'Result': result,\n",
    "            'League': league_name\n",
    "        })\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Match information not found for match ID: {match_id}\")\n",
    "\n",
    "# Convert the list of match information into a DataFrame\n",
    "matches_df = pd.DataFrame(matches_info)\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Match information successfully extracted.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "matches_df.to_csv('data/matches_info_2023_2024_1.csv', index=False)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Match information successfully extracted.\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4244785 # 4244791 First Game ID of the season\n",
    "end_match_id = 4244789 #4244838 # Adjust this according to your requirement\n",
    "\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "matches_info = []\n",
    "\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "\n",
    "    # Extract match information\n",
    "    try:\n",
    "        # Extract the home and away club names\n",
    "        league_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[1]/div/div[2]/h2/span/a').get_attribute(\"title\")\n",
    "        home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "        away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "        \n",
    "        # XPath for the result of the game\n",
    "        result_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/div/div/div')\n",
    "        result = result_element.text\n",
    "\n",
    "        # Append match information to the list\n",
    "        matches_info.append({\n",
    "            'Match ID': match_id,\n",
    "            'Home Team': home_club_name,\n",
    "            'Away Team': away_club_name,\n",
    "            'Result': result,\n",
    "            'League': league_name\n",
    "        })\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Match information not found for match ID: {match_id}\")\n",
    "\n",
    "# Convert the list of match information into a DataFrame\n",
    "matches_df = pd.DataFrame(matches_info)\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Match information successfully extracted.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "matches_df.to_csv('data/matches_info_2023_2024_2.csv', index=False)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The chromedriver version (121.0.6167.85) detected in PATH at C:\\Users\\moren\\Downloads\\chromedriver-win64\\chromedriver.exe might not be compatible with the detected chrome version (122.0.6261.129); currently, chromedriver 122.0.6261.128 is recommended for chrome 122.*, so it is advised to delete the driver in PATH and retry\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Iframe not found. Continuing after a couple of seconds...\n",
      "Match information successfully extracted.\n",
      "Webscraping successfully completed for all matches.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Chrome driver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Define the start and end match IDs\n",
    "start_match_id = 4244791 # 4244791 First Game ID of the season\n",
    "end_match_id = 4244838 #4244838 # Adjust this according to your requirement\n",
    "\n",
    "\n",
    "# Initialize an empty list to store all lineup stats dataframes\n",
    "matches_info = []\n",
    "\n",
    "\n",
    "# Loop through the range of match IDs\n",
    "for match_id in range(start_match_id, end_match_id + 1):\n",
    "    # Construct the URL for the current match ID\n",
    "    match_url = f\"https://www.transfermarkt.com/servette-fc_fc-lugano/aufstellung/spielbericht/{match_id}\"\n",
    "\n",
    "    # Navigate to the match URL\n",
    "    driver.get(match_url)\n",
    "\n",
    "    # Wait for page to load\n",
    "    time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        # Wait for the iframe to be present and switch to it\n",
    "        wait = WebDriverWait(driver, 1)\n",
    "        iframe = wait.until(EC.presence_of_element_located((By.ID, \"sp_message_iframe_953358\")))\n",
    "        driver.switch_to.frame(iframe)\n",
    "\n",
    "        # Now wait for the 'Accept & continue' button to be clickable inside the iframe\n",
    "        accept_button = wait.until(EC.element_to_be_clickable((By.XPATH, \"//button[contains(@class, 'accept')]\")))\n",
    "        accept_button.click()\n",
    "\n",
    "        # Switch back to the main document\n",
    "        driver.switch_to.default_content()\n",
    "\n",
    "    except TimeoutException:\n",
    "        # If the iframe doesn't appear, continue after a couple of seconds\n",
    "        print(\"Iframe not found. Continuing after a couple of seconds...\")\n",
    "        time.sleep(1)  # Adjust the time delay as needed\n",
    "\n",
    "\n",
    "    # Extract match information\n",
    "    try:\n",
    "        # Extract the home and away club names\n",
    "        league_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[1]/div/div[2]/h2/span/a').get_attribute(\"title\")\n",
    "        home_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[1]/a[2]').get_attribute(\"title\")\n",
    "        away_club_name = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[3]/a[2]').get_attribute(\"title\")\n",
    "        \n",
    "        # XPath for the result of the game\n",
    "        result_element = driver.find_element(By.XPATH, '//*[@id=\"main\"]/main/div[1]/div/div/div[2]/div[2]/div/div/div')\n",
    "        result = result_element.text\n",
    "\n",
    "        # Append match information to the list\n",
    "        matches_info.append({\n",
    "            'Match ID': match_id,\n",
    "            'Home Team': home_club_name,\n",
    "            'Away Team': away_club_name,\n",
    "            'Result': result,\n",
    "            'League': league_name\n",
    "        })\n",
    "\n",
    "    except NoSuchElementException:\n",
    "        print(f\"Match information not found for match ID: {match_id}\")\n",
    "\n",
    "# Convert the list of match information into a DataFrame\n",
    "matches_df = pd.DataFrame(matches_info)\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Match information successfully extracted.\")\n",
    "\n",
    "# Finally, save the dataframe to a CSV file for persistence\n",
    "matches_df.to_csv('data/matches_info_2023_2024_3.csv', index=False)\n",
    "\n",
    "# Close the driver after scraping is done\n",
    "driver.quit()\n",
    "\n",
    "# Print a success message after scraping all matches\n",
    "print(\"Webscraping successfully completed for all matches.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Match ID</th>\n",
       "      <th>Home Team</th>\n",
       "      <th>Away Team</th>\n",
       "      <th>Result</th>\n",
       "      <th>League</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4244791</td>\n",
       "      <td>FC Lausanne-Sport</td>\n",
       "      <td>Yverdon Sport FC</td>\n",
       "      <td>3:1\\n(3:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4244792</td>\n",
       "      <td>FC St. Gallen 1879</td>\n",
       "      <td>FC Winterthur</td>\n",
       "      <td>2:2\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4244793</td>\n",
       "      <td>Grasshopper Club Zurich</td>\n",
       "      <td>FC Basel 1893</td>\n",
       "      <td>2:1\\n(2:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4244794</td>\n",
       "      <td>BSC Young Boys</td>\n",
       "      <td>FC Stade-Lausanne-Ouchy</td>\n",
       "      <td>1:0\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4244795</td>\n",
       "      <td>FC Luzern</td>\n",
       "      <td>FC Zrich</td>\n",
       "      <td>0:1\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4244796</td>\n",
       "      <td>Servette FC</td>\n",
       "      <td>FC Lugano</td>\n",
       "      <td>2:1\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4244797</td>\n",
       "      <td>FC St. Gallen 1879</td>\n",
       "      <td>FC Stade-Lausanne-Ouchy</td>\n",
       "      <td>1:0\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4244798</td>\n",
       "      <td>Yverdon Sport FC</td>\n",
       "      <td>FC Basel 1893</td>\n",
       "      <td>0:2\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4244799</td>\n",
       "      <td>Grasshopper Club Zurich</td>\n",
       "      <td>FC Luzern</td>\n",
       "      <td>0:1\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4244800</td>\n",
       "      <td>FC Lugano</td>\n",
       "      <td>FC Zrich</td>\n",
       "      <td>2:0\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>4244801</td>\n",
       "      <td>FC Lausanne-Sport</td>\n",
       "      <td>FC Winterthur</td>\n",
       "      <td>1:1\\n(1:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4244802</td>\n",
       "      <td>BSC Young Boys</td>\n",
       "      <td>Servette FC</td>\n",
       "      <td>0:1\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4244803</td>\n",
       "      <td>Servette FC</td>\n",
       "      <td>FC St. Gallen 1879</td>\n",
       "      <td>2:0\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4244804</td>\n",
       "      <td>FC Stade-Lausanne-Ouchy</td>\n",
       "      <td>Grasshopper Club Zurich</td>\n",
       "      <td>1:1\\n(1:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4244805</td>\n",
       "      <td>FC Basel 1893</td>\n",
       "      <td>FC Lausanne-Sport</td>\n",
       "      <td>1:2\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4244806</td>\n",
       "      <td>FC Luzern</td>\n",
       "      <td>FC Lugano</td>\n",
       "      <td>0:1\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4244807</td>\n",
       "      <td>FC Winterthur</td>\n",
       "      <td>Yverdon Sport FC</td>\n",
       "      <td>2:1\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4244808</td>\n",
       "      <td>FC Zrich</td>\n",
       "      <td>BSC Young Boys</td>\n",
       "      <td>1:0\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4244809</td>\n",
       "      <td>FC Winterthur</td>\n",
       "      <td>Grasshopper Club Zurich</td>\n",
       "      <td>2:0\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4244810</td>\n",
       "      <td>Yverdon Sport FC</td>\n",
       "      <td>FC Zrich</td>\n",
       "      <td>3:2\\n(2:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4244811</td>\n",
       "      <td>FC St. Gallen 1879</td>\n",
       "      <td>FC Lugano</td>\n",
       "      <td>2:3\\n(2:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4244812</td>\n",
       "      <td>Servette FC</td>\n",
       "      <td>FC Lausanne-Sport</td>\n",
       "      <td>3:1\\n(1:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4244813</td>\n",
       "      <td>FC Stade-Lausanne-Ouchy</td>\n",
       "      <td>FC Luzern</td>\n",
       "      <td>2:1\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>4244814</td>\n",
       "      <td>BSC Young Boys</td>\n",
       "      <td>FC Basel 1893</td>\n",
       "      <td>5:1\\n(3:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4244815</td>\n",
       "      <td>FC Basel 1893</td>\n",
       "      <td>FC Winterthur</td>\n",
       "      <td>1:1\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>4244816</td>\n",
       "      <td>Grasshopper Club Zurich</td>\n",
       "      <td>FC St. Gallen 1879</td>\n",
       "      <td>1:1\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>4244817</td>\n",
       "      <td>FC Lugano</td>\n",
       "      <td>Yverdon Sport FC</td>\n",
       "      <td>2:0\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>4244818</td>\n",
       "      <td>FC Zrich</td>\n",
       "      <td>FC Stade-Lausanne-Ouchy</td>\n",
       "      <td>2:2\\n(2:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4244819</td>\n",
       "      <td>FC Lausanne-Sport</td>\n",
       "      <td>BSC Young Boys</td>\n",
       "      <td>2:0\\n(2:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4244820</td>\n",
       "      <td>FC Luzern</td>\n",
       "      <td>Servette FC</td>\n",
       "      <td>2:2\\n(1:2)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4244821</td>\n",
       "      <td>FC Stade-Lausanne-Ouchy</td>\n",
       "      <td>FC Lugano</td>\n",
       "      <td>1:3\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4244822</td>\n",
       "      <td>FC Winterthur</td>\n",
       "      <td>Servette FC</td>\n",
       "      <td>1:0\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4244823</td>\n",
       "      <td>FC Basel 1893</td>\n",
       "      <td>FC Zrich</td>\n",
       "      <td>2:2\\n(2:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4244824</td>\n",
       "      <td>Yverdon Sport FC</td>\n",
       "      <td>BSC Young Boys</td>\n",
       "      <td>0:0\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>4244825</td>\n",
       "      <td>Grasshopper Club Zurich</td>\n",
       "      <td>FC Lausanne-Sport</td>\n",
       "      <td>0:1\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>4244826</td>\n",
       "      <td>FC St. Gallen 1879</td>\n",
       "      <td>FC Luzern</td>\n",
       "      <td>1:1\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>4244827</td>\n",
       "      <td>FC Lugano</td>\n",
       "      <td>FC Basel 1893</td>\n",
       "      <td>2:0\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>4244828</td>\n",
       "      <td>Servette FC</td>\n",
       "      <td>FC Stade-Lausanne-Ouchy</td>\n",
       "      <td>1:2\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4244829</td>\n",
       "      <td>FC Zrich</td>\n",
       "      <td>FC Winterthur</td>\n",
       "      <td>0:0\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>4244830</td>\n",
       "      <td>FC Lausanne-Sport</td>\n",
       "      <td>FC St. Gallen 1879</td>\n",
       "      <td>3:3\\n(1:2)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>4244831</td>\n",
       "      <td>FC Luzern</td>\n",
       "      <td>Yverdon Sport FC</td>\n",
       "      <td>1:0\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4244832</td>\n",
       "      <td>BSC Young Boys</td>\n",
       "      <td>Grasshopper Club Zurich</td>\n",
       "      <td>3:0\\n(3:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>4244833</td>\n",
       "      <td>FC Stade-Lausanne-Ouchy</td>\n",
       "      <td>FC Basel 1893</td>\n",
       "      <td>0:2\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>4244834</td>\n",
       "      <td>FC Winterthur</td>\n",
       "      <td>FC Lugano</td>\n",
       "      <td>2:2\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>4244835</td>\n",
       "      <td>Servette FC</td>\n",
       "      <td>FC Zrich</td>\n",
       "      <td>0:1\\n(0:1)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4244836</td>\n",
       "      <td>FC Luzern</td>\n",
       "      <td>FC Lausanne-Sport</td>\n",
       "      <td>0:0\\n(0:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>4244837</td>\n",
       "      <td>FC St. Gallen 1879</td>\n",
       "      <td>BSC Young Boys</td>\n",
       "      <td>2:2\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>4244838</td>\n",
       "      <td>Yverdon Sport FC</td>\n",
       "      <td>Grasshopper Club Zurich</td>\n",
       "      <td>3:2\\n(1:0)</td>\n",
       "      <td>Super League</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Match ID                Home Team                Away Team      Result  \\\n",
       "0    4244791        FC Lausanne-Sport         Yverdon Sport FC  3:1\\n(3:1)   \n",
       "1    4244792       FC St. Gallen 1879            FC Winterthur  2:2\\n(1:0)   \n",
       "2    4244793  Grasshopper Club Zurich            FC Basel 1893  2:1\\n(2:1)   \n",
       "3    4244794           BSC Young Boys  FC Stade-Lausanne-Ouchy  1:0\\n(0:0)   \n",
       "4    4244795                FC Luzern                FC Zrich  0:1\\n(0:0)   \n",
       "5    4244796              Servette FC                FC Lugano  2:1\\n(0:1)   \n",
       "6    4244797       FC St. Gallen 1879  FC Stade-Lausanne-Ouchy  1:0\\n(0:0)   \n",
       "7    4244798         Yverdon Sport FC            FC Basel 1893  0:2\\n(0:1)   \n",
       "8    4244799  Grasshopper Club Zurich                FC Luzern  0:1\\n(0:0)   \n",
       "9    4244800                FC Lugano                FC Zrich  2:0\\n(1:0)   \n",
       "10   4244801        FC Lausanne-Sport            FC Winterthur  1:1\\n(1:1)   \n",
       "11   4244802           BSC Young Boys              Servette FC  0:1\\n(0:1)   \n",
       "12   4244803              Servette FC       FC St. Gallen 1879  2:0\\n(1:0)   \n",
       "13   4244804  FC Stade-Lausanne-Ouchy  Grasshopper Club Zurich  1:1\\n(1:1)   \n",
       "14   4244805            FC Basel 1893        FC Lausanne-Sport  1:2\\n(0:1)   \n",
       "15   4244806                FC Luzern                FC Lugano  0:1\\n(0:1)   \n",
       "16   4244807            FC Winterthur         Yverdon Sport FC  2:1\\n(1:0)   \n",
       "17   4244808                FC Zrich           BSC Young Boys  1:0\\n(1:0)   \n",
       "18   4244809            FC Winterthur  Grasshopper Club Zurich  2:0\\n(0:0)   \n",
       "19   4244810         Yverdon Sport FC                FC Zrich  3:2\\n(2:1)   \n",
       "20   4244811       FC St. Gallen 1879                FC Lugano  2:3\\n(2:1)   \n",
       "21   4244812              Servette FC        FC Lausanne-Sport  3:1\\n(1:1)   \n",
       "22   4244813  FC Stade-Lausanne-Ouchy                FC Luzern  2:1\\n(0:0)   \n",
       "23   4244814           BSC Young Boys            FC Basel 1893  5:1\\n(3:0)   \n",
       "24   4244815            FC Basel 1893            FC Winterthur  1:1\\n(0:1)   \n",
       "25   4244816  Grasshopper Club Zurich       FC St. Gallen 1879  1:1\\n(0:1)   \n",
       "26   4244817                FC Lugano         Yverdon Sport FC  2:0\\n(1:0)   \n",
       "27   4244818                FC Zrich  FC Stade-Lausanne-Ouchy  2:2\\n(2:1)   \n",
       "28   4244819        FC Lausanne-Sport           BSC Young Boys  2:0\\n(2:0)   \n",
       "29   4244820                FC Luzern              Servette FC  2:2\\n(1:2)   \n",
       "30   4244821  FC Stade-Lausanne-Ouchy                FC Lugano  1:3\\n(0:0)   \n",
       "31   4244822            FC Winterthur              Servette FC  1:0\\n(0:0)   \n",
       "32   4244823            FC Basel 1893                FC Zrich  2:2\\n(2:1)   \n",
       "33   4244824         Yverdon Sport FC           BSC Young Boys  0:0\\n(0:0)   \n",
       "34   4244825  Grasshopper Club Zurich        FC Lausanne-Sport  0:1\\n(0:1)   \n",
       "35   4244826       FC St. Gallen 1879                FC Luzern  1:1\\n(1:0)   \n",
       "36   4244827                FC Lugano            FC Basel 1893  2:0\\n(1:0)   \n",
       "37   4244828              Servette FC  FC Stade-Lausanne-Ouchy  1:2\\n(0:1)   \n",
       "38   4244829                FC Zrich            FC Winterthur  0:0\\n(0:0)   \n",
       "39   4244830        FC Lausanne-Sport       FC St. Gallen 1879  3:3\\n(1:2)   \n",
       "40   4244831                FC Luzern         Yverdon Sport FC  1:0\\n(1:0)   \n",
       "41   4244832           BSC Young Boys  Grasshopper Club Zurich  3:0\\n(3:0)   \n",
       "42   4244833  FC Stade-Lausanne-Ouchy            FC Basel 1893  0:2\\n(0:1)   \n",
       "43   4244834            FC Winterthur                FC Lugano  2:2\\n(0:0)   \n",
       "44   4244835              Servette FC                FC Zrich  0:1\\n(0:1)   \n",
       "45   4244836                FC Luzern        FC Lausanne-Sport  0:0\\n(0:0)   \n",
       "46   4244837       FC St. Gallen 1879           BSC Young Boys  2:2\\n(1:0)   \n",
       "47   4244838         Yverdon Sport FC  Grasshopper Club Zurich  3:2\\n(1:0)   \n",
       "\n",
       "          League  \n",
       "0   Super League  \n",
       "1   Super League  \n",
       "2   Super League  \n",
       "3   Super League  \n",
       "4   Super League  \n",
       "5   Super League  \n",
       "6   Super League  \n",
       "7   Super League  \n",
       "8   Super League  \n",
       "9   Super League  \n",
       "10  Super League  \n",
       "11  Super League  \n",
       "12  Super League  \n",
       "13  Super League  \n",
       "14  Super League  \n",
       "15  Super League  \n",
       "16  Super League  \n",
       "17  Super League  \n",
       "18  Super League  \n",
       "19  Super League  \n",
       "20  Super League  \n",
       "21  Super League  \n",
       "22  Super League  \n",
       "23  Super League  \n",
       "24  Super League  \n",
       "25  Super League  \n",
       "26  Super League  \n",
       "27  Super League  \n",
       "28  Super League  \n",
       "29  Super League  \n",
       "30  Super League  \n",
       "31  Super League  \n",
       "32  Super League  \n",
       "33  Super League  \n",
       "34  Super League  \n",
       "35  Super League  \n",
       "36  Super League  \n",
       "37  Super League  \n",
       "38  Super League  \n",
       "39  Super League  \n",
       "40  Super League  \n",
       "41  Super League  \n",
       "42  Super League  \n",
       "43  Super League  \n",
       "44  Super League  \n",
       "45  Super League  \n",
       "46  Super League  \n",
       "47  Super League  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
